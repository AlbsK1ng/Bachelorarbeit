\newpage
\section{Entwicklung}
\label{sec:Entwicklung}
In diesem Kapitel wird die schrittweise Entwicklung des digitalen Zwillings für das Abfüll- und Verschließmodul der robocell-Linie dargestellt.
Ziel ist es, das physische Asset gemäß dem Konzept der \acs{aas} digital abzubilden und in ein Industrie-4.0-konformes System zu integrieren.
Abbildung \ref{fig:Entwicklungsschritte} veranschaulicht den Ablauf des zugrunde liegenden Entwicklungsprozesses.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{Bilder/vorgehenEntwicklungsteil.pdf}
    \caption[Entwicklungsprozess des digitalen Zwillings]{Entwicklungsprozess des digitalen Zwillings}
    \label{fig:Entwicklungsschritte}
\end{figure}
\vspace{-0.5em}

Aufbauend auf einer konzeptionellen Grundlage erfolgt zunächst die statische Modellierung der \acs{aas} mithilfe des Package Explorers.
Die resultierende Struktur wird anschließend mit einer Test Engine validiert.
Danach wird sie über die Eclipse BaSyx-Plattform bereitgestellt und um dynamische Informationen erweitert, etwa durch die Anbindung von Echtzeitdaten über \acs{opcua}.

Nach der Implementierung des digitalen Zwillings wird ein \acs{ki}-Modell zur Analyse und Optimierung der darin enthaltenen Daten entwickelt und prototypisch umgesetzt. 
Darüber hinaus werden zwei exemplarische Anwendungsfälle betrachtet. 
Im ersten steht die Realisierung eines \acs{dpp} im Vordergrund, wobei der Fokus auf der strukturierten Abbildung des \ac{pcf} sowie der Implementierung differenzierter Zugriffsrechte liegt. 
Der zweite Anwendungsfall zeigt, wie \acs{aas}-Instanzen automatisiert generiert und bereitgestellt werden können.

\subsection{Konzeptionierung des digitalen Zwillings}
Ziel dieses Abschnitts ist es, eine Basis für die Erstellung des digitalen Zwillings zu schaffen. 
Untersucht wird, welche Daten für die Modellierung erforderlich sind, woher sie stammen und wie sie in Submodellen der \acs{aas} strukturiert werden können.

\subsubsection{Identifikation relevanter Datenquellen}
Ein digitaler Zwilling basiert immer auf einer Vielzahl unterschiedlicher Daten, die gemeinsam ein umfassendes digitales Abbild eines Assets ermöglichen. 
Dabei werden sowohl statische Informationen (z.~B. Datenblätter oder Konstruktionsdaten) als auch%
\pagebreak
~dynamische Daten, die während des Betriebs einer Maschine anfallen, benötigt.
Im ersten Schritt gilt es daher, alle relevanten Datenquellen zu identifizieren, die für die Modellierung des digitalen Zwillings erforderlich sind.

In industriellen Umgebungen kommen typischerweise verschiedene Systeme zur Erfassung, Verwaltung und Speicherung von Maschinendaten zum Einsatz.
Bei groninger übernimmt diese Funktion das \acs{plm}-System Agile, das eng mit dem \acs{erp}-System PSI Penta verknüpft ist.
Darin sind unter anderem Stücklisten, technische Spezifikationen, \acs{cad}-Dateien sowie allgemeine Dokumente hinterlegt, welche die statische Grundlage für den digitalen Zwilling bilden.

Neben den Informationen aus den Unternehmenssystemen spielen aber auch Laufzeitdaten, wie sie durch Sensoren oder Steuerungssysteme erzeugt werden, eine zentrale Rolle.
Da im Rahmen dieser Arbeit keine reale Maschine angebunden ist, werden diese Daten simuliert.
Hierfür kommt eine in Node.js entwickelte Anwendung zum Einsatz, die im Folgenden als Datengenerator bezeichnet wird und sowohl Prozess- als auch Betriebsdaten generiert. 

Ergänzend dazu wird ein Maschinensimulator verwendet, der einen \ac{packml}-Zustands\-automaten abbildet und typische Maschinenzustände sowie deren Übergänge simuliert. 
Beide Komponenten stehen als Docker-Container zur Verfügung und stellen die Daten über einen \acs{opcua} Server bereit, wodurch eine realitätsnahe Datenbasis geschaffen wird.
\subsubsection{Auswahl geeigneter Teilmodelle}
Aufbauend auf den zuvor betrachteten Informationsquellen gilt es nun zu entscheiden, welche Aspekte der Maschine im digitalen Zwilling, oder besser gesagt in der \acs{aas}, abgebildet werden sollen.
Im nächsten Schritt ist daher die Auswahl bzw. der Entwurf geeigneter Submodelle erforderlich, die die relevanten Informationen strukturiert bereitstellen.

Als Orientierung dienen die von der \acs{idta} bereitgestellten \acsp{smt} \cite{idtaTemplates}, die bereits viele typische Anwendungsfälle standardisiert abdecken.
Diese sind jeweils in einer Submodellspezifikation der \acs{idta} dokumentiert.
Darüber hinaus besteht auch die Möglichkeit, eigene Submodelle zu entwerfen, die gezielt auf projektspezifische Anforderungen zugeschnitten sind.
Diese können entweder vollständig neu konzipiert oder aus bestehenden Vorlagen abgeleitet werden.

Die konkrete Auswahl der Submodelle in dieser Arbeit orientiert sich hauptsächlich an typischen Industrie-4.0-Anwendungsfällen, die unter anderem auf der Website der \acs{idta} dokumentiert sind \cite{idtaUseCases}.
Diese Anwendungsfälle verdeutlichen, welche Submodelle in der Praxis besonders relevant sind.
Eines der wichtigsten ist das digitale Typenschild, da dieses häufig die erste Anlaufstelle für die Identifikation und grundlegende Informationen eines Assets darstellt.
Daneben wurden aber auch projektspezifische Anforderungen berücksichtigt, die sich aus den verfügbaren Daten sowie dem fachlichen Austausch mit Industriepartnern wie Wittenstein ergaben.

Tabelle \ref{tab:Submodelle} liefert einen Überblick über die initiale Auswahl dieser Submodelle sowie den zugehörigen Datenquellen.
Diese werden in späteren Anwendungsfällen gezielt erweitert.
Zur besseren Übersicht sind dynamische Submodelle farblich hervorgehoben, während statische Submodelle weiß hinterlegt sind.
Sofern vorhanden, verweist die Spalte Standardisierung auf das jeweils zugehörige \acs{smt} mittels der offiziellen Dokumentennummer der \acs{idta}, in der die betreffende Version spezifiziert ist.
Die Spalte Vorgesehene Inhalte zeigt beispielhaft, welche Informationen im weiteren Verlauf im digitalen Zwilling abgebildet werden sollen.

\vspace{0.15em}
\input{TabelleSubmodelle.tex}

\newpage
Im Rahmen dieser Arbeit ist die Modellierung der \acs{aas} bzw. der verschiedenen Submodelle als Instanz vorgesehen.
Auch wenn es sich bei dem Abfüll- und Verschließmodul grundsätzlich um ein generisches Modul innerhalb der robocell-Linie handelt und somit technisch gesehen als Typ klassifiziert werden könnte, steht in diesem Projekt die Umsetzung eines konkreten digitalen Zwillings im Vordergrund.
Dies begründet sich vor allem durch die vorgesehene Anbindung von Echtzeit- und Zeitreihendaten sowie die Abbildung einer Betriebsumgebung, was dem digitalen Zwilling einen klaren Instanzcharakter verleiht.
Zudem erleichtert dies die spätere Demonstration der \acs{aas} im Industrie-4.0-Systemkontext.

\subsection{Modellierung mit der AAS}

Im Folgenden werden die zuvor ausgewählten Submodelle in eine \acs{aas} integriert und um konkrete Daten sowie semantische Informationen ergänzt.
Dabei wird gezeigt, wie sich die \acs{aas} von Grund auf modellieren, schrittweise mit Inhalten füllen und anschließend speichern sowie exportieren lässt.

Die Umsetzung erfolgt mithilfe des Package Explorers, der eine intuitive Modellierung aller relevanten Elemente unterstützt und so den strukturierten Aufbau des digitalen Zwillings ermöglicht.
Im Anschluss wird die erstellte \acs{aas} mit einer Test Engine auf Korrektheit und Vollständigkeit überprüft.

\subsubsection{Praktische Umsetzung mit dem Package Explorer}

Die Modellierung beginnt mit der Erstellung eines neuen \acs{aas}-Pakets, das als \mbox{Container} für die digitalen Inhalte eines Assets dient.
In diesem Paket kann eine neue \acs{aas} angelegt werden, die sowohl allgemeine als auch assetspezifische Informationen enthält.
Abbildung~\ref{fig:NeuesAASPaket} zeigt den initialen Aufbau im \mbox{Package Explorer}, der im weiteren Verlauf sukzessive um Submodelle und deren Inhalte erweitert wird.
Bereits zu Beginn lassen sich Dateien, etwa ein Titelbild des Abfüll- und Verschließmoduls, einbinden, die damit integraler Bestandteil des digitalen Zwillings sind.

Im nächsten Schritt sind die Metadaten der \acs{aas} sowie des zugehörigen Assets festzulegen.
Neben der Auswahl des Asset-Typs spielt vor allem die eindeutige Identifikation eine zentrale Rolle.
Das Asset wird über eine globalAssetId referenziert, während die \acs{aas} eine eigene \acs{id} und eine idShort erhält.
Diese erleichtern nicht nur den späteren Austausch, sondern ermöglichen auch das systemweite Auffinden innerhalb eines Industrie-4.0-Ökosystems.
Für erste Modellierungszwecke empfiehlt es sich, auf automatisch generierte Beispiel-IDs zurückzugreifen.

\newpage
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{Bilder/ModellierungAAS/Final/AASPaketPackageExplorer.PNG}
    \caption[Initiales \acs{aas}-Paket im Package Explorer]{Initiales \acs{aas}-Paket im Package Explorer}
    \label{fig:NeuesAASPaket}
\end{figure}



\subsubsection*{Strukturierung durch Submodelle}
\vspace{-0.5em}

Als Nächstes müssen die benötigten Submodelle zur \acs{aas} hinzugefügt werden.
Diese lassen sich entweder als Instanz, Typ oder Template anlegen, was eine flexible Modellierung je nach Anwendungsfall ermöglicht.
Jedes Submodell erhält, wie auch die \acs{aas}, eine eindeutige \acs{id}, über die es später identifiziert werden kann.
Zusätzlich können optionale Administrationsinformationen angegeben werden, wie beispielsweise die Versionsnummer oder der Name des Autors.

Beim Erstellen ist ein Submodell zunächst leer. 
Die Modellierung erfolgt durch das schrittweise Hinzufügen von Submodellelementen.
Tabelle~\ref{tab:Submodellelemente} gibt einen Überblick über die wichtigsten Elemente, die im Package Explorer zur Verfügung stehen.

\vspace{0.5em}
\input{TabelleSubmodellElemente}
\vspace{-0.5em}

Wie in der Tabelle dargestellt, unterscheidet die Metamodellspezifikation \cite{SpezifikationPart1} zwischen DataElements und SubmodelElements.
DataElements bilden stets die unterste Ebene des Modells. 
Sie enthalten konkrete Informationen wie Werte oder Dateien und können keine weiteren Elemente umfassen.
SubmodelElements hingegen sind komplexere Strukturen, die als Container für weitere untergeordnete Elemente dienen.

Im Gegensatz zu Submodellen erhalten Submodellelemente keine eigene \acs{id}.
Sie werden ausschließlich über ihre idShort referenziert, die innerhalb eines Submodells eindeutig sein muss.
Eine Ausnahme bilden \acsp{sml}, deren Elemente nicht über die idShort, sondern ausschließlich über ihre Position (Index) identifiziert werden.

Alternativ zur manuellen Modellierung können die im Rahmen der Konzeption ausgewählten \acsp{smt} genutzt werden, die in Tabelle~\ref{tab:Submodelle} aufgeführt sind.
Diese stammen beispielsweise aus dem offiziellen Repository der \acs{idta} \cite{idtaTemplates} und können als AASX-Dateien importiert werden.
Im Package Explorer erfolgt der Import über ein Auxiliary AAS, das als sekundäre Umgebung zur Template-Verwaltung dient.

Die Templates enthalten strukturierte Vorlagen mit typischen Submodellelementen und verringern dadurch den Modellierungsaufwand erheblich.
Darauf aufbauend lassen sich ebenfalls unternehmensspezifische Ableitungen erstellen.
Ein vertiefter Ansatz wird in Kapitel~\ref{chap:ErstellenvonSubmodelTemplates} vorgestellt.

Nach dem Strukturaufbau können konkrete Inhalte wie Werte, Dokumente oder Referenzen eingepflegt werden.
Dies betrifft jedoch ausschließlich statische Submodelle wie Technische Daten oder Dokumentation.
Dynamische Inhalte, etwa Betriebsdaten, werden hingegen nicht direkt im Package Explorer gepflegt, sondern zur Laufzeit über externe Datenquellen (Maschinensimulator und Datengenerator) angebunden.

\subsubsection*{Semantische Beschreibung mit Concept Descriptions}
\vspace{-0.5em}

Für die semantische Beschreibung von Submodellen und Submodellelementen innerhalb der \acs{aas} ist die Vergabe von semanticId-Referenzen von zentraler Bedeutung.
Jede \mbox{semanticId} verweist auf eine zugehörige \acs{cd}, welche die Bedeutung des referenzierten Elements eindeutig und maschinenlesbar beschreibt.
Im Package Explorer lassen sich solche Concept Descriptions manuell anlegen und verwalten.
Hierfür steht eine Eingabemaske zur Verfügung, die sich an dem in Part 3a: Data Specification - IEC 61360 \cite{SpezifikationPart3a} definierten Datenmodell für semantische Beschreibungen orientiert.

Die Grundstruktur dieser Vorlage ist bei allen Elementen der \acs{aas} identisch.
Je nach Elementtyp unterscheiden sich jedoch die verpflichtenden und empfohlenen Felder.
So gelten für unterschiedliche Elemente (z.~B. Property oder RelationshipElement) jeweils spezifische Pflichtfelder, während andere Angaben optional oder nicht vorgesehen sind.
Abbildung~\ref{fig:SubmodellTypenschild} veranschaulicht die Eingabemaske, wobei die Struktur exemplarisch anhand einer Property für die physikalische Größe Frequenz dargestellt ist.

\vspace{0.25em}
\begin{figure}[htbp]
    \centering
    \input{DataSpecificationContent.tex}
    \vspace{-0.5em}
    \caption[Eingabemaske einer \acs{cd}]{Eingabemaske einer \acs{cd} (in Anlehnung an \cite{SpezifikationPart3a})}
    \label{fig:SubmodellTypenschild}
\end{figure}
\vspace{-0.35em}

Alternativ können auch externe Standards referenziert werden. 
Der Package Explorer unterstützt diesen Ansatz durch eine erweiterte Funktionalität, mit der vorgefertigte ECLASS-Kataloge in das Tool importiert werden können. 
Nach dem Import lassen sich die enthaltenen Objekte durchsuchen, auswählen und den entsprechenden Submodellen oder Submodellelementen zuweisen. 
Die semantische Verknüpfung erfolgt über eine \acs{irdi}, die das jeweilige Konzept eindeutig identifiziert \cite{eclass_irdi}.

Wird ein solcher Standard referenziert, legt der Package Explorer automatisch eine \acs{cd} an. 
Sie folgt, analog zur manuellen Erstellung, dem in IEC~61360 definierten Datenmodell. 
Somit sind die semantischen Informationen auch lokal im Modell verfügbar und konsistent strukturiert. 
Technisch betrachtet wäre jedoch auch eine rein externe Referenz ausreichend, da die entsprechenden Elemente bereits vollständig in den jeweiligen Katalogen enthalten sind.

Die Nutzung etablierter externer Standards ist grundsätzlich zu bevorzugen, da sie nicht nur den Modellierungsaufwand erheblich reduziert, sondern auch die Interoperabilität zwischen verschiedenen Systemen und Anwendungen deutlich verbessert.

\subsubsection*{Exportmöglichkeiten im Package Explorer}
\vspace{-0.5em}
Nach Abschluss der Modellierung kann die erstellte \acs{aas} in verschiedenen Formaten exportiert werden.
Das bevorzugte Format in diesem Projekt ist das AASX-Format, das sich als standardisierte Austauschform für die \acs{aas} etabliert hat.
Es umfasst alle modellierten Inhalte, einschließlich eingebetteter Dateien wie Bilder oder Dokumente, und eignet sich daher besonders für die Weitergabe einer Typ-1-\acs{aas}.

Alternativ kann die \acs{aas} auch als \ac{json}- oder \ac{xml}-Datei gespeichert werden.
Sie enthalten ausschließlich die strukturierte Beschreibung der \acs{aas}, sind jedoch besonders für die Nutzung in \acsp{api} oder zur Anbindung an bestehende Softwaresysteme geeignet.

Darüber hinaus ermöglicht der Package Explorer den gezielten Export einzelner Concept Descriptions oder \mbox{Submodelle}.
Diese können separat, beispielsweise als \acs{json}-Datei, gespeichert und in anderen \acs{aas}-Projekten wiederverwendet werden.

\subsubsection{Validierung}

Im Anschluss an die Modellierung sollte eine Überprüfung der Konformität erfolgen. 
Dazu kann die von der \acs{idta} bereitgestellte Test Engine \cite{TestEngine} verwendet werden. 
Sie lässt sich mit pip, dem Paketmanager von Python, installieren und anschließend über die Kommandozeile ausführen. 
Zwar bietet auch der Package Explorer eine integrierte Validierungsfunktion, diese ist jedoch nur eingeschränkt nutzbar und eignet sich daher nicht für eine umfassende Prüfung.

Mit dem Befehl 
\tcbox[inlinebox]{{aas\_test\_engine check\_files~<dateiname>.aasx}} 
kann die Validierung der zuvor erstellten \acs{aas} gestartet werden.
Unterstützt werden sowohl AASX- als auch \acs{json}-Dateien. 
Dabei wird zunächst geprüft, ob diese formal korrekt aufgebaut sind, insbesondere hinsichtlich der internen Strukturen und Beziehungen. 
Anschließend erfolgt die Kontrolle der \acs{aas} anhand der Metamodell-Spezifikationen (Teil~1 \cite{SpezifikationPart1} und 3a \cite{SpezifikationPart3a}). 
Zuletzt werden die Submodelle mit den zugehörigen \acsp{smt} abgeglichen, sofern diese für das jeweilige Submodell definiert sind.

\newpage
Treten bei der Validierung formale oder semantische Fehler auf, beispielsweise durch fehlende Referenzen oder ungültige IDs, gibt die Test Engine detaillierte Fehlermeldungen in der Konsole aus. 
Die Ausgaben geben Aufschluss darüber, an welcher Stelle die Struktur bzw. der Inhalt der geprüften Datei fehlerhaft ist, und ermöglichen so eine gezielte Korrektur der betroffenen Elemente. 
Werden im gesamten Prüfprozess hingegen keine Fehler oder Abweichungen festgestellt, bestätigt die Test Engine die erfolgreiche Validierung.

\subsection{Technische Integration}
Im Anschluss an die Konzeption und Modellierung des digitalen Zwillings steht in diesem Kapitel die technische Integration in eine Industrie-4.0-kompatible Umgebung im Mittelpunkt. 
Es wird gezeigt, wie die statisch modellierte Typ-1-\acs{aas} mithilfe des AASX Server Blazor beziehungsweise vorzugsweise der Eclipse BaSyx-Plattform in eine Typ-2-\acs{aas} überführt und systemseitig bereitgestellt werden kann. 
Darüber hinaus wird dargestellt, wie die \acs{aas} um dynamische Inhalte erweitert werden kann. 
Dies umfasst die Integration von Echtzeitdaten über \acs{opcua} sowie die Einbindung und Visualisierung externer Zeitreihendaten mithilfe von InfluxDB und Telegraf.

\subsubsection{Bereitstellung der \acs{aas}}
\label{sec:bereitstellungAAS}
Für die Bereitstellung als Typ-2-\acs{aas} stehen verschiedene Open-Source-Lösungen zur Verfügung. 
Eine besonders einsteigerfreundliche Option ist der AASX Server Blazor \cite{AASXServer}.
Er bildet das serverseitige Gegenstück zum Package Explorer und verfügt ebenfalls über eine grafische Benutzeroberfläche zur Visualisierung von \acs{aas}-Paketen.

Über eine \acs{http}-Schnittstelle können beide Anwendungen miteinander verbunden werden.
Dadurch lassen sich AASX-Dateien ohne zusätzlichen Konfigurationsaufwand direkt aus dem Package Explorer heraus auf den Server übertragen und bereitstellen.
Ebenso können auf dem Server gespeicherte \acs{aas} über den Explorer eingesehen und bearbeitet werden.
Die enge Verzahnung beider Komponenten ermöglicht somit eine unkomplizierte Bereitstellung und Verwaltung von \acs{aas}-Paketen und eignet sich besonders für erste Testszenarien oder prototypische Anwendungen.

Für komplexere Anwendungsszenarien, insbesondere solche mit Anforderungen an Echtzeitfähigkeit sowie an flexible und erweiterbare Submodelle, stößt der AASX Server Blazor jedoch an seine funktionalen und architektonischen Grenzen. 
So fehlen beispielsweise Schnittstellen zur Anbindung dynamischer Datenquellen sowie Möglichkeiten zur Skalierung und zur verteilten Systemintegration.

\pagebreak

Aus diesem Grund wird im weiteren Verlauf dieses Projekts die Eclipse BaSyx-Plattform eingesetzt.
Durch ihre modulare Architektur und die klare Trennung verschiedener Komponenten wie den Registries oder den 
Repositories bietet sie eine deutlich flexiblere Grundlage für die Umsetzung anspruchsvoller Industrie-4.0-Szenarien.

Die einfachste Möglichkeit, BaSyx zu installieren, besteht in der Nutzung von Docker.
Alle benötigten Komponenten stehen als vorgefertigte Images öffentlich über den Docker Hub \cite{BaSyxDockerHub} zur Verfügung. 
Alternativ kann der Quellcode von GitHub \cite{BaSyxGithub} bezogen werden, um einzelne Komponenten individuell anzupassen oder zu erweitern.
Für die vorliegende Arbeit ist dies insbesondere im Zusammenhang mit der AAS Web UI von Vorteil, da sich benutzerdefinierte Plugins flexibel integrieren lassen.

Die verschiedenen Services, darunter die AAS Web UI, die \acs{aas} Environment, Registries für \acs{aas} und Submodelle, der Discovery Service, sowie die MongoDB als persistenter Speicher, können zentral in einer docker-compose.yml-Datei verwaltet werden.
Die Konfiguration erfolgt über Umgebungsvariablen sowie separate Konfigurationsdateien, die in der docker-compose.yml referenziert werden.

Nach erfolgreichem Start der BaSyx-Umgebung stehen verschiedene Möglichkeiten zur Verfügung, um eine \acs{aas} bereitzustellen und in das System zu integrieren. 
Welche Methode zum Einsatz kommt, hängt von den jeweiligen Anforderungen und Rahmenbedingungen des Anwendungsszenarios ab.
Nachfolgend werden drei ausgewählte Ansätze zur Bereitstellung näher betrachtet, die in dieser Arbeit Anwendung finden.

\vspace{0.5em}
\noindent\textbf{A. Volume}\\[0.5em]
Eine besonders einfache Variante besteht darin, eine AASX-Datei in ein gemountetes \mbox{Volume} der AAS Environment abzulegen.
Ein Volume ist ein persistentes Speicherverzeichnis auf dem Host-System, das mit einem Verzeichnis innerhalb eines Docker-Containers verknüpft ist.
Es ermöglicht die dauerhafte Speicherung von Daten, unabhängig vom Lebenszyklus des Containers.
In der von Eclipse BaSyx bereitgestellten Docker-Umgebung ist ein entsprechendes Volume in der Regel bereits vorkonfiguriert.

Beim Neustart der AAS Environment wird die AASX-Datei automatisch erkannt, registriert und in das BaSyx-System eingebunden.
Die dabei erzeugten Daten, darunter Informationen zu \acs{aas}, Submodellen, Concept Descriptions sowie Registrierungsdaten, werden in verschiedenen Tabellen der angebundenen MongoDB-Instanz gespeichert.
Dies gewährleistet, dass alle relevanten Daten persistent erhalten bleiben, auch wenn die ursprüngliche AASX-Datei später wieder aus dem Volume entfernt wird.

\clearpage
\noindent\textbf{B. AAS Web UI}\\[0.5em]
Alternativ kann die Bereitstellung auch direkt über die AAS Web UI erfolgen.
Über die Benutzeroberfläche kann eine AASX-Datei manuell importiert werden, wodurch sie unmittelbar im laufenden System registriert und eingebunden wird.
Diese Methode eignet sich besonders gut für Tests oder kleinere Anpassungen, da eine \acs{aas} auf diese Weise schnell und ohne direkten Zugriff auf das zugrunde liegende Dateisystem integriert werden kann.

\vspace{0.5em}
\noindent\textbf{C. REST-API}\\[0.5em]
Die flexibelste, zugleich jedoch technisch anspruchsvollste Methode zur Bereitstellung ist die manuelle Registrierung über die \acs{rest}-\acs{api}. 
Dabei kann nicht einfach eine AASX-Datei hochgeladen werden. 
Stattdessen müssen die \acs{aas}, ihre Submodelle sowie deren Beziehungen explizit über die bereitgestellten Schnittstellen des BaSyx-Systems erstellt werden. 
Dies erfolgt durch das Übermitteln strukturierter \acs{json}-Daten im Body der jeweiligen \acs{http}-Anfragen.

Ein typischer Ablauf dieser Registrierung ist in Tabelle \ref{tab:BereitstellungInBaSyx} dargestellt. 
Sie zeigt die notwendigen Schritte sowie die zugehörigen \acs{rest}-Endpunkte, die den jeweiligen Repositories der AAS Environment zugeordnet sind.

\vspace{0.25em}
\input{TabelleBereitstellungBaSyxMitAPI.tex}

Neben der Bereitstellung in der AAS Environment kann eine \acs{aas} optional auch im Discovery Service registriert werden.
Über einen sogenannten assetLink lässt sich diese logisch mit dem zugehörigen physischen Asset verknüpfen.
Die Registrierung erfolgt analog zur AAS Environment über einen \acs{rest}-Endpunkt.
Dies erleichtert die eindeutige Zuordnung, insbesondere in komplexen, hierarchisch aufgebauten \acs{aas}, die wiederum mehrere verschachtelte Assets digital abbilden.

\newpage
\subsubsection{Integration von Echtzeitdaten über OPC UA}

Nach der Erstellung der statischen \acs{aas} des Abfüll- und Verschließmoduls sowie deren Integration in das BaSyx-System soll diese nun um dynamische Informationen ergänzt werden.
Diese sind erforderlich, um den aktuellen Maschinenzustand präzise und in Echtzeit abzubilden.
Grundlage bilden die beiden zuvor beschriebenen Anwendungen, welche simulierte Maschinen- und Sensordaten über einen \acs{opcua} Server bereitstellen.

Die Integration der Echtzeitdaten wird im Folgenden am Beispiel des \mbox{Submodells} Prozessdaten erläutert.
Wie in Abbildung \ref{fig:SubmodellProzessdaten} dargestellt, enthält dieses Submodell verschiedene Properties, die jeweils spezifische Werte, etwa den Druck oder die Anzahl abgefüllter Einheiten, repräsentieren.
Diese sollen im weiteren Verlauf dynamisch mit den über \acs{opcua} bereitgestellten Werten aktualisiert werden.

\begin{figure}[htbp]
    \centering
    % 0.88
    \includegraphics[width=1\textwidth]{Bilder/OPCUA/ProcessData.pdf}
    \caption[Struktur Submodell Prozessdaten]{Struktur Submodell Prozessdaten}
    \label{fig:SubmodellProzessdaten}
\end{figure}
\vspace{-0.5em}

Das Eclipse BaSyx-Projekt stellt hierfür eine weitere Komponente bereit, die Databridge \cite{BaSyxDatabridge}.
Diese steht, wie alle anderen Komponenten, als Docker-Container zur Verfügung und ermöglicht die Anbindung verschiedenster Datenquellen an eine \acs{aas}.
Sie unterstützt eine Vielzahl von Protokollen, darunter insbesondere \acs{opcua} und \acs{mqtt} (Message Queuing Telemetry Transport).
Dabei dient sie als Vermittler zwischen einem Datenendpunkt und einem Submodell innerhalb der \acs{aas}.

Die Konfiguration erfolgt über mehrere \acs{json}-Dateien.
In einer zentralen Konfigurationsdatei werden sowohl die Datenquelle als auch die Datensenke definiert.
Zusätzlich können Transformatoren angegeben werden, die beispielsweise Einheitenumrechnungen oder Typkonvertierungen übernehmen.

Im Kontext des Submodells Prozessdaten erfolgt die Anbindung über den \acs{opcua} Server des Datengenerators, der als Datenquelle für die Databridge dient.
Zu diesem Zweck wird eine separate \acs{json}-Datei verwendet.
Darin sind sowohl die Verbindungsparameter des Servers (z.~B. \acs{url} (Uniform Resource Locator) und Port) als auch die zu überwachenden Knoten anzugeben.
Wie in Abbildung \ref{fig:OPCUADatenStruktur} zu erkennen, stellt der \acs{opcua} Server die Werte hierfür in einer hierarchischen Struktur bereit, wobei jeder Knoten über einen Namespaceindex (ns) und eine NodeId (i) eindeutig adressierbar ist.

\newpage
\begin{figure}[htbp]
    \centering
    % 0.88
    \includegraphics{Bilder/OPCUA/OPCUADaten.pdf}
    \caption[\acs{opcua} Datenknoten des Datengenerators]{\acs{opcua} Datenknoten des Datengenerators}
    \label{fig:OPCUADatenStruktur}
\end{figure}

Zusätzlich muss der zu verwendende \acs{opcua} Client festgelegt werden.
In diesem Projekt kommt Eclipse Milo zum Einsatz, der auf einem Subscription-Modell basiert.
Im Gegensatz zu einem Polling-Ansatz werden hierbei gezielt bestimmte Knoten abonniert, sodass neue Werte bei Änderungen automatisch übermittelt werden.

Ein Beispiel für eine entsprechende Konfiguration der Datenquelle ist in Listing~\ref{lst:jsonDatenquelle} für den Druckwert dargestellt.
Weitere optionale Parameter, wie etwa Sicherheitseinstellungen oder das Übertragungsintervall, können ebenfalls angegeben werden, wurden hier jedoch zur besseren Übersicht weggelassen.

\begin{lstlisting}[language=json, caption={\acs{json}-Konfiguration einer Datenquelle}, label={lst:jsonDatenquelle}]
{
    "uniqueId"       : "pressure",
    "nodeInformation": "ns=4;i=113",
    "serverUrl"      : "opcua-server",
    "serverPort"     : 4840,
    "pathToService"  : "milo"
}
\end{lstlisting}

Zur Anpassung der eingehenden Daten an die Struktur der Ziel-Property können unterschiedliche Transformatoren eingesetzt werden.
Die Databridge unterstützt unter anderem JSONata-Ausdrücke sowie Jackson-basierte Transformer.
Mithilfe dieser Mechanismen lassen sich die vom \acs{opcua} Server empfangenen Rohdaten in ein \acs{json}-Objekt überführen, gezielt extrahieren und weiterverarbeiten.

Im Anschluss erfolgt die Konfiguration der Datensenke, also der Zielkomponente innerhalb der \acs{aas}.
Diese wird ebenfalls über eine separate \acs{json}-Datei definiert, in der der Endpunkt des Submodells, der idShortPath der gewünschten Property sowie die verwendete \acs{api}-Version angegeben werden.
Listing~\ref{lst:jsonDatensenke} zeigt ein Beispiel einer solchen Konfiguration.
Der Platzhalter \{smId\} steht für die Base64-kodierte \acs{id} des Submodells Prozessdaten, wie sie in der \acs{rest}-API der \acs{aas} verwendet wird.

\newpage
\begin{lstlisting}[language=json, caption={\acs{json}-Konfiguration einer Datensenke}, label={lst:jsonDatensenke}]
{
    "uniqueId"        : "Submodel/ProcessData/Pressure",
    "submodelEndpoint": "http://aas-env:8081/submodels/{smId}",
    "idShortPath"     : "Data.Pressure",
    "api"             : "DotAAS-V3"
}
\end{lstlisting}

Nach erfolgreicher Konfiguration übernimmt die Databridge die automatisierte Übertragung der \acs{opcua} Werte in die entsprechenden Properties des Submodells.
Dieses Prinzip lässt sich nicht nur auf Prozesswerte anwenden, sondern auch zur Abbildung des Maschinenzustands oder anderer dynamischer Betriebsdaten nutzen.
Die Databridge stellt damit eine zentrale Komponente zur Echtzeitanbindung externer Datenquellen dar und trägt wesentlich zur dynamischen Erweiterung der \acs{aas} bei.

\subsubsection{Verarbeitung von Zeitreihendaten}
\label{sec: VerarbeitungZeitreihen}
Grundsätzlich lassen sich Zeitreihendaten auf unterschiedliche Weise in eine \acs{aas} integrieren.
Das \acs{smt} Time Series Data \cite{SpezifikationTimeSeriesData} bietet hierfür mehrere standardisierte Lösungsansätze.
Eine Möglichkeit besteht darin, die Daten direkt innerhalb der \acs{aas} zu speichern. 
Dies geschieht über ein InternalSegment, das die entsprechenden Einträge enthält, eignet sich jedoch nur für kleinere Datenmengen.

Alternativ können die Zeitreihendaten in Form einer Datei abgelegt werden. 
Diese kann entweder direkt in die \acs{aas} eingebunden oder über ein ExternalSegment referenziert werden.
Für größere Datenmengen empfiehlt sich die externe Speicherung an einem separaten Ort, beispielsweise in einer Datenbank.
Die Verknüpfung mit der \acs{aas} erfolgt in diesem Fall über ein LinkedSegment.

In dieser Arbeit wird die zuletzt genannte Option näher untersucht und praktisch umgesetzt.
Hierzu werden die simulierten Werte für Druck und Temperatur des Datengenerators extern in einer InfluxDB gespeichert.
Die über \acs{opcua} bereitgestellten Daten werden mithilfe von Telegraf, einem leichtgewichtigen Agenten zur Datenerfassung und -weiterleitung \cite{Influx}, kontinuierlich in eine speziell für diesen Anwendungsfall angelegte Tabelle geschrieben.
InfluxDB und Telegraf stehen beide als Docker-Container zur Verfügung und lassen sich nahtlos in das bestehende BaSyx-System integrieren.

Zur Einbindung der in der Datenbank gespeicherten Daten in die \acs{aas} wird das zuvor eingeführte \acs{smt} genutzt.
Abbildung \ref{fig:SMTTimeSeriesData} zeigt den strukturellen Aufbau, wobei ausschließlich die für die externe Anbindung relevanten Elemente berücksichtigt sind.

\begin{figure}[htbp]
    \centering
    % 0.88
    \includegraphics[width=1\textwidth]{Bilder/TimeSeries/TimeSeriesData.pdf}
    \caption[Struktur \acs{smt} Time Series Data]{Struktur \acs{smt} Time Series Data (in Anlehnung an \cite{SpezifikationTimeSeriesData})}
    \label{fig:SMTTimeSeriesData}
\end{figure}

Zunächst werden die Metadaten eingetragen. 
Dazu gehören ein eindeutiger Name, eine Beschreibung sowie die Festlegung der Struktur der aufzuzeichnenden Datenpunkte in der \acs{smc} Records.
Ein Record kann mit einer Spaltenbeschreibung in einer Tabelle verglichen werden.
Er beschreibt, welche Variablen, in diesem Fall Druck und Temperatur, ein Zeitreihendatensatz enthält und wie diese zu interpretieren sind.

Die Konfiguration des LinkedSegments bildet den nächsten Schritt.
Dieses stellt die eigentliche Verbindung zu den extern gespeicherten Zeitreihendaten her. 
Hierfür sind sowohl der Datenendpunkt als auch die Abfrage zu spezifizieren, mit der die gewünschten Werte ausgelesen werden können. 
Im vorliegenden Fall handelt es sich um die Adresse des InfluxDB-Containers sowie die Abfrage, mit der die Werte für Druck und Temperatur aus der zugehörigen Tabelle abgerufen werden. 
Dabei ist zu beachten, dass der entsprechende Port der InfluxDB nach außen freigegeben sein muss, da andernfalls kein Zugriff durch externe Anwendungen möglich wäre.

Zusätzlich können weitere Metadaten angegeben werden, beispielsweise die Abtastrate, der durch das Segment abgedeckte Zeitraum oder ein RecordCount, der die erwartete Anzahl an Einträgen innerhalb dieses Zeitfensters beschreibt.
Alle relevanten Informationen zur externen Speicherung der Zeitreihendaten sind somit im Submodell enthalten, wodurch externen Clients ein strukturierter und standardisierter Zugang zu den Daten ermöglicht wird.

\newpage
\subsection{KI-Modell zur Optimierung}
In diesem Abschnitt wird ein Konzept zur Optimierung der zustandsbasierten Prozessüberwachung auf Basis von \acs{ki} vorgestellt und prototypisch umgesetzt.
Ziel ist es, mithilfe maschinellen Lernens Anomalien in den im digitalen Zwilling abgebildeten Prozessdaten zu erkennen.
Damit wird eine Grundlage für proaktive Instandhaltungsmaßnahmen geschaffen, die zur Optimierung des Betriebs einer Maschine beitragen können.

\subsubsection{Konzeptidee}

Mit der fortschreitenden Digitalisierung industrieller Prozesse entstehen zunehmend umfangreiche Datenmengen, die wertvolle Informationen über den Zustand und das Verhalten eines Assets liefern. 
Dabei handelt es sich um unterschiedlichste Datenarten, etwa die Drehzahl eines Motors oder, wie im Rahmen dieser Arbeit im digitalen Zwilling abgebildet, klassische Prozessdaten wie Temperatur und Druck.
Die manuelle Analyse dieser Daten ist jedoch sehr anspruchsvoll, da Abweichungen vom Normalbetrieb oft schwer zu erkennen sind und dadurch eine zuverlässige Zustandsüberwachung erheblich erschwert wird.

Ein vielversprechender Ansatz, der diese Problematik adressiert, ist der sogenannte \mbox{Autoencoder}. 
Dabei handelt es sich um eine spezielle Architektur tiefer neuronaler Netze, die typischerweise aus zwei Hauptkomponenten besteht: einem Encoder und einem Decoder, die gemeinsam eine komprimierte, latente Repräsentation der Eingangsdaten erlernen \cite{Lempitsky2019}.

Der prinzipielle Aufbau eines Autoencoders ist in Abbildung \ref{fig:Autoencoder} dargestellt.

\vspace{0.5em}
\begin{figure}[htbp]
    \centering
    % 0.88
    \includegraphics[width=1\textwidth]{Bilder/Autoencoder/AutoencoderModell.pdf}
    \caption[Aufbau eines Autoencoders]{Aufbau eines Autoencoders (in Anlehnung an \cite{AutoencoderBild})}
    \label{fig:Autoencoder}
\end{figure}

Der Encoder besteht aus mehreren Schichten, die die Eingabedaten schrittweise in eine komprimierte Form überführen. 
Dabei erfolgt eine Dimensionsreduktion, das heißt, die Daten werden im Verlauf auf weniger Dimensionen verdichtet. 
Das Ziel ist, die für die Rekonstruktion wesentlichen Merkmale zu extrahieren und irrelevante Informationen zu filtern.

Der dabei entstehende latente Raum bildet die am stärksten komprimierte Repräsentation der Daten ab und dient als Ausgangspunkt für den Decoder.
Dieser übernimmt die Aufgabe, die komprimierte Darstellung in die ursprüngliche Eingabe zu rekonstruieren. 
Auch er besteht aus mehreren Schichten, die die Daten schrittweise dekomprimieren.
Zur Bewertung der Leistungsfähigkeit kann die resultierende Ausgabe mit der ursprünglichen Eingabe verglichen werden. 
Die Differenz zwischen beiden wird als Rekonstruktionsfehler bezeichnet. \cite{Autoencoder}

Im Rahmen dieser Arbeit soll ein Autoencoder zur Erkennung von Anomalien in Zeitreihendaten eingesetzt werden. 
Geplant ist ein Training mit Daten aus dem Normalbetrieb. 
Da diese nicht gelabelt sind, wird ein unüberwachter Lernansatz verfolgt. 
Ziel ist es, dass das Modell typische Muster des Normalbetriebs eigenständig erkennt und rekonstruieren kann. 
Weichen neue Eingangsdaten deutlich vom gelernten Muster ab, steigt der Rekonstruktionsfehler.
Dieser dient als Indikator für eine Anomalie.

\subsubsection{Prototypische Umsetzung}
Für die prototypische Umsetzung des Autoencoders wird die Programmiersprache Python verwendet. 
Aufgrund ihrer Vielzahl an spezialisierten Frameworks für maschinelles Lernen gilt sie als Standardwerkzeug für datengetriebene Analysen \cite{Python}.
Als Datengrundlage dienen die simulierten Temperaturverläufe, die mithilfe des Datengenerators erzeugt und, wie auch in Kapitel \ref{sec: VerarbeitungZeitreihen} beschrieben, in einer InfluxDB gespeichert sind.

Zunächst werden die für das Training notwendigen Daten vorbereitet.
Hierzu wird ein InfluxDB-Client eingesetzt, der über ein \acs{api}-Token sowie die \acs{url} des Containers mit der Datenquelle kommuniziert.
Die relevanten Temperaturdaten lassen sich anschließend durch gezielte Abfragen aus der entsprechenden Tabelle extrahieren.
In dieser Umsetzung wird ein Zeitraum von sieben Tagen betrachtet.

Um dem Autoencoder das Erkennen typischer Muster innerhalb dieser Daten zu ermöglichen, werden sie in Segmente unterteilt.
Die Wahl der Fenstergröße ist hierbei entscheidend, da sie bestimmt, in welchem Maß Prozessänderungen erfasst werden können.
Da diese in der Regel in kurzen Zeitabschnitten auftreten, muss eine Fensterlänge gewählt werden, die ausreichend Kontext liefert, ohne zu große Zeiträume zusammenzufassen.

Die resultierenden Sequenzen werden anschließend in einem Tensor zusammengeführt. 
Dabei handelt es sich um ein mehrdimensionales Array, in diesem Fall eine zweidimensionale Matrix, bei der jede Zeile ein Zeitfenster mit einer definierten Anzahl an Temperaturwerten abbildet. 
Diese Struktur ist speziell für \acs{ki}-Modelle ausgelegt und erlaubt dem Autoencoder, typische Muster in den Zeitreihendaten zu erlernen.

Der Autoencoder selbst wird mit dem Deep-Learning-Framework PyTorch \cite{PyTorch} implementiert.
Der Encoder komprimiert die Eingabesequenz schrittweise von 64 auf 16 und schließlich auf 4 Neuronen, welche den latenten Raum repräsentieren.
Der Decoder ist spiegelbildlich aufgebaut und rekonstruiert aus dem latenten Raum die ursprüngliche Eingabesequenz, indem er die Dimensionen in umgekehrter Reihenfolge auf 16, 64 und schließlich auf die ursprüngliche Sequenzlänge erweitert.

Als Eingabe für das Training dienen die zuvor definierten Tensoren.
Ziel ist, die Eingabedaten möglichst exakt zu rekonstruieren.
Nach jedem Trainingsdurchgang wird der mittlere quadratische Fehler zwischen Eingabe- und Ausgabedaten berechnet und zur Anpassung der Netzgewichte mittels Backpropagation verwendet.
Durch die iterative Wiederholung dieses Prozesses über mehrere Trainingszyklen (Epochen) wird der Rekonstruktionsfehler sukzessive verringert und somit die Fähigkeit des Modells, die Eingabedaten genau nachzubilden, kontinuierlich verbessert.

Das trainierte Modell kann anschließend zur Erkennung von Anomalien in neuen Daten genutzt werden.
Dabei dient der Rekonstruktionsfehler nicht mehr zur Optimierung, sondern als Maß für Abweichungen von der Normalität.
Überschreitet der Fehler eines Zeitfensters einen definierten Schwellenwert, so wird eine Anomalie detektiert.

% \begin{lstlisting}[language=json, caption={\acs{json}-Konfiguration einer Datensenke}, label={lst:jassonDatensenke}]
% {
% class Autoencoder(nn.Module):
%     def __init__(self, window_size):
%         super(Autoencoder, self).__init__()
%         self.encoder = nn.Sequential(
%             nn.Linear(window_size, 64),
%             nn.ReLU(),
%             nn.Linear(64, 16),
%             nn.ReLU(),
%             nn.Linear(16, 4),
%             nn.ReLU()
%         )
%         self.decoder = nn.Sequential(
%             nn.Linear(4, 16),
%             nn.ReLU(),
%             nn.Linear(16, 64),
%             nn.ReLU(),
%             nn.Linear(64, window_size),
%             nn.ReLU()
%         )

%     def forward(self, x):
%         x = self.encoder(x)
%         x = self.decoder(x)
%         return x
% }
% \end{lstlisting}

\newpage
% Seitenumbruch im Inhaltsverzeichnis
% \addtocontents{toc}{\protect\newpage}
\subsection{Anwendungsfall Digitaler Produktpass}
Angesichts steigender Anforderungen an Nachhaltigkeit und Transparenz über den gesamten Produktlebenszyklus gewinnt der \acs{dpp} zunehmend an Bedeutung.
Aufbauend auf dem von \acs{zvei} und \acs{idta} vorgestellten Konzept des \acs{dpp40}, das unter anderem in einem \acs{pcf}-Showcase \cite{PCFShowcas} demonstriert wird, wird in diesem Anwendungsfall untersucht, wie sich zentrale Herausforderungen mithilfe der \acs{aas} umsetzen lassen.
Im Mittelpunkt stehen die Erfassung des \acs{pcf} sowie die Realisierung differenzierter Zugriffsrechte auf die im \acs{dpp} enthaltenen Informationen.
Grundlage bildet die zuvor erstellte \acs{aas} des Abfüll- und Verschließmoduls, die um ein Submodell zur Abbildung des \acs{cf} erweitert wird.

\subsubsection{Umsetzung mit dem \acs{smt} Carbon Footprint}
Der \acs{pcf} beschreibt die Summe aller Treibhausgasemissionen, ausgedrückt in CO\textsubscript{2}-Äqui\-valenten, die entlang des Lebenszyklus eines Produkts entstehen \cite{PCF}. 
Seine Relevanz zeigt sich unter anderem in der \acs{eu}-Batterieverordnung \cite{EUVerordnung}, die die Einführung eines digitalen Batteriepasses vorschreibt. 
Dieser stellt die erste konkrete Umsetzung eines \acs{dpp} dar und dient als Ausgangspunkt für weitere Branchen wie den Maschinen- und Anlagenbau. 
Die Verordnung fordert unter anderem die verpflichtende Angabe des \acs{pcf} für die Phasen Material, Produktion und die Gesamtbetrachtung (Cradle to Gate). 
Für weiterführende Lebenszyklusphasen ist die Angabe derzeit noch nicht vorgeschrieben.

Für die Umsetzung bietet sich das von der \acs{idta} spezifizierte \acs{smt} \acs{cf} \cite{SpezifikaitonPCF} an.
Es definiert eine standardisierte Struktur zur Erfassung von CO\textsubscript{2}-Äquivalenten und unterscheidet zwischen dem \acs{pcf} und dem \ac{tcf}, wobei in diesem Anwendungsfall ausschließlich der \acs{pcf} betrachtet wird.
Die Modellierung erfolgt über \acsp{smc}, die standardisierte Elemente wie den CO\textsubscript{2}-Wert, die betrachtete Lebenszyklusphase, die Berechnungsmethode sowie den Gültigkeitszeitraum enthalten.
Für jede relevante Phase empfiehlt es sich, eine eigene \acs{smc} anzulegen.
Die konkreten Werte bleiben zunächst leer und werden im weiteren Verlauf dynamisch gefüllt.

Zur Demonstration wurde eine Auswahl von acht im Abfüll- und Verschließmodul verbauten Siemens-Steuerungskomponenten getroffen.
Dazu gehören beispielsweise eine CPU sowie Ein- und Ausgangsmodule.
Die Entscheidung für Siemens basiert darauf, dass für deren Produkte bereits \acs{aas}-Strukturen mit nachhaltigkeitsbezogenen Informationen vorliegen, auch wenn diese derzeit noch nicht öffentlich verfügbar sind.
Die zugehörigen \acs{aas} wurden daher im Rahmen des Projekts auf persönliche Anfrage von der Siemens AG bereitgestellt.
Diese enthalten jeweils ein eigenes Submodell gemäß dem \acs{smt} \acs{cf}.

\clearpage
Die Referenzierung der Komponenten findet im \acs{cf}-Submodell des Abfüll- und Verschließmoduls, nachfolgend als Haupt-\acs{aas} bezeichnet, statt.
Dies erfolgt über eine \acs{sml}, in der die ausgewählten Einheiten als Self-Managed Entities unter Angabe ihrer globalAssetId eingebunden sind.
Self-Managed Entities sind eigenständige Assets, die über eine eigene \acs{aas} verfügen und somit unabhängig adressierbar sind.
Zwar wäre auch eine Referenzierung über das \acs{bom}-Submodell denkbar, jedoch wären die relevanten Daten dadurch auf verschiedene Submodelle verteilt, was die Aggregation erschweren würde.

Die aktualisierte Haupt-\acs{aas} sowie die zugehörigen Komponenten-\acs{aas} werden über die Eclipse~BaSyx-Plattform bereitgestellt und bilden die Grundlage für die technische Umsetzung.
Für die Benutzerinteraktion steht ein Vue.js-Plugin der AAS Web UI zur Verfügung, das um eine Schaltfläche zur Initialisierung erweiterbar ist.

Zur Trennung von Logik und Benutzeroberfläche wird ein eigenständiger, Node.js-basierter Microservice implementiert, der eine Web-\acs{api} bereitstellt.
Diese lässt sich vom Vue.js-Plugin aufrufen und löst die Berechnung der CO\textsubscript{2}-Äquivalente aus.
Die Aggregation erfolgt damit zentral und serverseitig, anstatt direkt im Plugin clientseitig.

Der Microservice liest die im \acs{cf}-Submodell des Abfüll- und Verschließmoduls referenzierten Komponenten aus, aggregiert deren \acs{pcf}-Werte und schreibt die berechneten CO\textsubscript{2}-Äquivalente in die vorbereiteten \acsp{smc} der Haupt-\acs{aas} zurück.
Zusätzlich können weitere Metadaten wie die verwendete Berechnungsmethode oder der Gültigkeitszeitraum berücksichtigt werden.
Da diese Werte im vorliegenden Anwendungsfall jedoch bei allen Komponenten identisch sind, ist keine Differenzierung erforderlich.

\subsubsection{Zugriffsrechte und Datensicherheit}

Die kontrollierte Bereitstellung der im \acs{dpp} enthaltenen Informationen ist essenziell, insbesondere im Hinblick auf Datenschutz, regulatorische Vorgaben sowie den Schutz geistigen Eigentums.  
Wie in Kapitel~\ref{sec: Sicherheit} beschrieben, sieht die Security-Spezifikation der \acs{idta} hierfür die Nutzung eines attributbasierten Zugriffskontrollmodells (\acs{abac}) vor.

Da die \acs{abac}-Funktionalität in aktuellen Referenzimplementierungen wie dem AASX Server Blazor oder Eclipse BaSyx jedoch noch nicht vollständig verfügbar ist, wird im Rahmen dieses Anwendungsfalls exemplarisch ein rollenbasiertes Zugriffsmodell (\acs{rbac}) umgesetzt, wie es von der Eclipse BaSyx-Plattform unterstützt wird.  
Im Gegensatz zum \acs{abac}-Ansatz erfolgt die Zugriffskontrolle hierbei durch die Zuweisung spezifischer Berechtigungen an vordefinierte Rollen, denen wiederum Benutzer oder technische Clients zugeordnet sind.

Zur Umsetzung von Authentifizierung und Autorisierung wird der Open-Source Identity Provider Keycloak eingesetzt \cite{Keycloak}.  
Dieser ermöglicht die zentrale Verwaltung von Benutzern, Rollen und Clients und unterstützt eine tokenbasierte Zugriffskontrolle auf Basis von \acsp{jwt}.  
Keycloak steht als Docker-Container zur Verfügung und kann nahtlos in die BaSyx-Systemarchitektur integriert werden.

Im ersten Schritt muss ein dediziertes Realm eingerichtet werden, das als gekapselte Umgebung für alle projektbezogenen Identitäten dient.  
Innerhalb dieses Realms lassen sich sowohl Benutzerkonten als auch Rollen definieren, wobei die Zuweisung über sogenannte Role Mappings erfolgt.  
Ein Benutzer kann einer oder mehreren Rollen gleichzeitig zugeordnet sein.

Jedes Benutzerkonto verfügt über einen eindeutigen Benutzernamen sowie ein ini\-tiales Passwort zur Authentifizierung.  
Zur Erhöhung der Sicherheit kann dieses Passwort als temporär markiert werden, wodurch der Benutzer bei der ersten Anmeldung zu einer Änderung gezwungen wird.  
Darüber hinaus bietet Keycloak die Möglichkeit, sogenannte Required Actions (erforderliche Aktionen) zu definieren, etwa zur E-Mail-Verifizierung oder erzwungenen Passwortänderung beim nächsten Login.  
So lässt sich der Authentifizierungsprozess flexibel an projektspezifische Sicherheitsanforderungen anpassen.

Neben der Verwaltung menschlicher Benutzer unterstützt Keycloak auch die Administration technischer Clients, die z.~B. maschinelle Anwendungen oder externe Systeme repräsentieren.  
Die Authentifizierung erfolgt in der Regel über eine Kombination aus eindeutiger Client-\acs{id} und einem geheimen Schlüssel (Client Secret) gemäß dem OpenID Connect-Protokoll \cite{OpenID}.  
Für jeden Client kann optional ein Service Account aktiviert werden, der als technisches Benutzerkonto agiert und, analog zu realen Benutzern, mit spezifischen Rollen ausgestattet ist.  
Dies ermöglicht eine differenzierte Zugriffssteuerung auch für automatisierte Systemzugriffe.

Die Rechtevergabe innerhalb der einzelnen BaSyx-Komponenten erfolgt rollenbasiert über externe Konfigurationsdateien im \acs{json}-Format.  
Darin wird definiert, welche Rollen welche Berechtigungen (z.~B. READ, WRITE, DELETE) auf bestimmte Elemente wie \acs{aas}, Submodelle oder Concept Descriptions erhalten.  
Für generische Regeln kann der Platzhalter \texttt{*} verwendet werden, um beispielsweise den Zugriff auf alle Submodelle einer BaSyx-Komponente zu gewähren.  
Die AAS Web UI benötigt keine eigene \acs{rbac}-Datei, sondern orientiert sich an den Konfigurationen der angebundenen Services.

\subsection{Anwendungsfall automatisierte Generierung von AAS}
In diesem Anwendungsfall wird gezeigt, wie eine \acs{aas} automatisiert erzeugt und bereitgestellt werden kann.  
Ausgangspunkt ist ein unternehmensspezifisch angepasstes \acs{smt}, das die Basis für die Ableitung eines Typ-Submodells bildet.
Dieses wird durch ein Skript mit Daten befüllt, in eine vollständige \acs{aas}-Instanz überführt und abschließend in das \mbox{Eclipse BaSyx-System} integriert.

\subsubsection{Arbeiten mit Submodel Templates}
\label{chap:ErstellenvonSubmodelTemplates}
Für die automatisierte Generierung ist eine konsistente Submodellstruktur erforderlich, die sich aus wiederverwendbaren Vorlagen ableiten lässt.
Das standardisierte \acs{smt} \mbox{Generic} \mbox{Frame} \mbox{for} \mbox{Technical} \mbox{Data} \mbox{for} \mbox{Industrial} \mbox{Equipment} \mbox{in} \mbox{Manufacturing} \cite{SpezifikaitonTechnischeDaten} dient in diesem Anwendungsfall als Grundlage.
Es definiert eine generische Struktur zur Beschreibung technischer Merkmale, gegliedert in Kategorien (\acsp{smc}) wie Generelle Informationen, Technische Informationen oder \mbox{Produktklassifikation}.
Damit stellt es den semantischen Rahmen bereit, enthält jedoch keine konkreten Ausprägungen.

Auf Unternehmensebene lässt sich die generische Vorlage an spezifische Anforderungen anpassen, beispielsweise für einen bestimmten Maschinen- oder Anlagentyp wie die robocell. 
Branchenspezifische Varianten sind ebenfalls denkbar, etwa wenn für eine gesamte Maschinenklasse wie Abfüllmaschinen auf Basis des standardisierten \acs{smt} ein von Fachverbänden abgestimmtes Template abgeleitet wird.
In dieser Arbeit werden solche branchenspezifischen Ansätze jedoch nicht weiter betrachtet.

Die Anpassung erfolgt in dieser Arbeit mit dem Package Explorer.
Über ihn lässt sich das generische \acs{smt} importieren und erweitern.
So können produktspezifische Anforderungen, wie Umgebungsbedingungen oder der Verarbeitungsbereich, mithilfe geeigneter Submodellelemente ergänzt werden.
Das resultierende unternehmensspezifische Template erhält eine eigene \mbox{semanticId}, bleibt jedoch zugleich mit der semanticId des generischen Templates verknüpft.
Dadurch ist die Rückverfolgbarkeit zur standardisierten Vorlage gewährleistet.

Aus dem angepassten \acs{smt} kann anschließend ein Typ-Submodell abgeleitet werden, das nicht nur die Struktur, sondern auch allgemeingültige Merkmale einer Produktgruppe enthält, etwa den Einsatzort oder die maximal zulässige Umgebungstemperatur.
Es dient als Vorlage für konkrete Instanz-Submodelle, die durch die Befüllung mit produktspezifischen Werten wie der Seriennummer oder der Art der zulässigen Behältnisse entstehen.
Die Verbindung zur ursprünglichen sowie zur unternehmensspezifischen Template-Struktur bleibt über entsprechende semanticId-Verknüpfungen erhalten.

\subsubsection{Automatisiertes Befüllen mit strukturierten Daten}

Nach der Ableitung eines Typ-Submodells muss dieses effizient mit Produktinformationen befüllt und in eine vollständige \acs{aas}-Instanz überführt werden. 
Die hierfür benötigten Daten liegen in der industriellen Praxis häufig bereits strukturiert in bestehenden \mbox{\acs{it}-Systemen} wie \acs{plm}- oder \acs{erp}-Lösungen vor.

Da eine direkte Systemintegration im Rahmen dieser Arbeit nicht realisierbar ist, wird der Prozess anhand vordefinierter Beispieldaten demonstriert.
Diese enthalten typische technische Merkmale, wie sie auch in realen Unternehmenssystemen anzutreffen sind, und bilden die Grundlage für die automatisierte Erstellung eines Instanz-Submodells.

Die automatisierte Befüllung erfolgt mithilfe eines Skripts, das die Beispieldaten in die vorgegebene Submodellstruktur überträgt und in eine \acs{aas}-Instanz einbettet.
Zum Einsatz kommt die serverseitige JavaScript-Laufzeitumgebung Node.js \cite{nodejs}, da sie eine effiziente Verarbeitung von \acs{json}-Daten sowie eine unkomplizierte Kommunikation mit \acs{rest}-Schnittstellen ermöglicht.

Grundlage bilden drei strukturierte \acs{json}-Dateien:

\vspace{0.2em}
\begin{enumerate}[noitemsep, parsep=0.2em, label=\textbf{\arabic*.}, labelsep=0.4em]
    \item \makebox[2.9cm][l]{\textbf{Datenquelle}}    Beinhaltet technische Produktinformationen
    \item \makebox[2.9cm][l]{\textbf{Typ-Submodell}}  Definiert Struktur und Semantik des Submodells
    \item \makebox[2.9cm][l]{\textbf{AAS-Vorlage}}    Beschreibt Aufbau und Struktur der \acs{aas}-Instanz
\end{enumerate}
\vspace{0.2em}

Die Datenquelle ist hierarchisch aufgebaut und besteht aus verschachtelten Schlüssel-Wert-Paaren.
Jeder Schlüssel entspricht dem idShort-Wert eines Submodellelements innerhalb des Typ-Submodells.
Dieses enthält Platzhalter, die zur Laufzeit automatisch durch die zugehörigen Werte aus der Datenquelle ersetzt werden.

Das resultierende Instanz-Submodell muss anschließend in eine \acs{aas} eingebunden werden. 
Hierfür wird die \acs{aas}-Vorlage verwendet, die die grundlegende Struktur vorgibt, jedoch ohne Angaben wie die \acs{id} der \acs{aas} oder des zugehörigen Assets. 
Diese Identifikatoren lassen sich während der Skriptausführung durch zufällig generierte \ac{uuid}-Werte an den entsprechenden Stellen einfügen.
Dadurch erhält jede \acs{aas} eine eindeutige Kennung und wird zu einer vollständigen Instanz.

Im letzten Schritt muss die \acs{aas} gemeinsam mit dem zugehörigen Submodell innerhalb des Eclipse BaSyx-Systems bereitgestellt werden. 
Hierzu können beide über die \acs{rest}-\acs{api} eingebunden und verknüpft werden, analog zur in Kapitel~\ref{sec:bereitstellungAAS} beschriebenen Möglichkeit~C.
Durch die Ausführung des Skripts entsteht so automatisch eine Typ-2-\acs{aas}.
