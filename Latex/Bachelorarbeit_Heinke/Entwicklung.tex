\newpage
\section{Entwicklung}
\label{sec:Entwicklung}
In diesem Kapitel wird die schrittweise Entwicklung des digitalen Zwillings für das Abfüll- und Verschließmodul der robocell-Linie dargestellt.
Ziel ist es, das physische Asset gemäß dem Konzept der \acs{aas} digital abzubilden und in ein Industrie-4.0-konformes System zu integrieren.
Abbildung \ref{fig:Entwicklungsschritte} veranschaulicht den Ablauf des zugrundeliegenden Entwicklungsprozesses.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{Bilder/vorgehenEntwicklungsteil.pdf}
    \caption[Entwicklungsprozess des digitalen Zwillings]{Entwicklungsprozess des digitalen Zwillings}
    \label{fig:Entwicklungsschritte}
\end{figure}
\vspace{-0.5em}

Aufbauend auf einer konzeptionellen Grundlage erfolgt zunächst die statische Modellierung der \acs{aas} mithilfe des Package Explorers. 
Die resultierende Struktur wird anschließend mit einer Test Engine validiert.
Im weiteren Verlauf wird die \acs{aas} mit der Eclipse BaSyx-Plattform bereitgestellt und um dynamische Informationen ergänzt. 
Dabei werden sowohl Echtzeitdaten über \acs{opcua} als auch Zeitreihendaten über eine InfluxDB eingebunden.

Nach der Implementierung des digitalen Zwillings wird ein \acs{ki}-Modell zur Analyse und Optimierung der darin enthaltenen Daten vorgestellt und prototypisch umgesetzt.
Zudem wird die Entwicklung von zwei exemplarischen Anwendungsfällen betrachtet.
Im ersten Anwendungsfall steht die Umsetzung eines \acs{dpp} im Vordergrund. 
Der Fokus liegt auf der strukturierten Abbildung des \ac{pcf} sowie der Implementierung differenzierter Zugriffsrechte auf die verschiedenen Bestandteile.
Der zweite Anwendungsfall zeigt, wie \acs{aas}-Instanzen automatisiert generiert und bereitgestellt werden können.

\subsection{Konzeptionierung des digitalen Zwillings}
Ziel dieses Abschnittes ist es, eine grundlegende Basis für die Erstellung des digitalen Zwillings zu schaffen.
Dabei wird untersucht, welche Daten für die Modellierung erforderlich sind, wo diese herkommen und wie sie in (standardisierten) Submodellen der \acs{aas} strukturiert werden können.
\subsubsection{Identifikation relevanter Datenquellen}
Ein digitaler Zwilling basiert immer auf einer Vielzahl unterschiedlicher Daten, die gemeinsam ein umfassendes digitales Abbild eines Assets ermöglichen. 
Dabei werden%
\pagebreak 
~sowohl statische Informationen (z.B. Datenblätter oder Konstruktionsdaten) als auch dynamische Daten, die während des Betriebs einer Maschine anfallen, benötigt.
Im ersten Schritt gilt es daher, alle relevanten Datenquellen zu identifizieren, die für die Modellierung des digitalen Zwillings erforderlich sind.

In industriellen Umgebungen kommen typischerweise verschiedene Systeme zur Erfassung, Verwaltung und Speicherung von Maschinendaten zum Einsatz.
Bei groninger übernimmt diese Funktion das \acs{plm}-System Agile, das eng mit dem \acs{erp}-System PSI Penta verknüpft ist.
Darin sind unter anderem Stücklisten, technische Spezifikationen, \acs{cad}-Dateien sowie allgemeine Dokumente hinterlegt, die die statische Grundlage  für den digitalen Zwilling bilden.

Neben den Informationen aus den Unternehmenssystemen spielen aber auch Laufzeitdaten, wie sie durch Sensoren oder Steuerungssysteme erzeugt werden, eine zentrale Rolle.
Da im Rahmen dieser Arbeit keine reale Maschine angebunden ist, werden diese Daten simuliert.
Hierfür kommt eine in Node.js entwickelte Anwendung zum Einsatz, die im Folgenden als Datengenerator bezeichnet wird und sowohl Prozess- als auch Betriebsdaten generiert. 

Ergänzend dazu wird ein Maschinensimulator verwendet, der einen PackML-Zustands\-automaten abbildet und typische Maschinenzustände sowie deren Übergänge simuliert. 
Beide Komponenten stehen als Docker-Container zur Verfügung und stellen die Daten über einen \acs{opcua} Server bereit, wodurch eine realitätsnahe Datenbasis geschaffen wird.
\subsubsection{Auswahl geeigneter Teilmodelle}
Aufbauend auf den zuvor betrachteten Informationsquellen gilt es nun zu entscheiden, welche Aspekte der Maschine im digitalen Zwilling, oder besser gesagt in der \acs{aas}, abgebildet werden sollen.
Im nächsten Schritt ist daher die Auswahl bzw. der Entwurf geeigneter Submodelle erforderlich, die die relevanten Informationen strukturiert bereitstellen.

Als Orientierung dienen die von der \acs{idta} bereitgestellten \acsp{smt} \cite{idtaTemplates}, die bereits viele typische Anwendungsfälle standardisiert abdecken.
Diese sind jeweils in einer Submodellspezifikation der \acs{idta} dokumentiert.
Darüber hinaus besteht jedoch auch die Möglichkeit, eigene Submodelle zu entwerfen, die gezielt auf projektspezifische Anforderungen zugeschnitten sind.
Diese können entweder vollständig neu konzipiert oder aus bestehenden Vorlagen abgeleitet werden.

Die konkrete Auswahl der Submodelle in dieser Arbeit orientiert sich hauptsächlich an typischen Industrie-4.0-Anwendungsfällen, die unter anderem auf der Website der \acs{idta} dokumentiert sind \cite{idtaUseCases}.
Diese Anwendungsfälle zeigen auf, welche Submodelle in der Praxis besonders relevant sind.
Eines der wichtigsten ist vermutlich das digitale Typenschild, da dieses häufig die erste Anlaufstelle für die Identifikation und grundlegende Informationen eines Assets darstellt.
Daneben wurden aber auch projektspezifische Anforderungen berücksichtigt, die sich aus den verfügbaren Daten sowie dem fachlichen Austausch mit Industriepartnern wie Wittenstein ergaben.

Tabelle \ref{tab:Submodelle} liefert einen Überblick über die initiale Auswahl dieser Submodelle sowie den zugehörigen Datenquellen.
Diese werden in späteren Anwendungsfällen gezielt erweitert.
Zur besseren Übersicht sind dynamische Submodelle farblich hervorgehoben, während statische Submodelle weiß hinterlegt sind.
Sofern vorhanden, verweist die Spalte Standardisierung auf das jeweils zugehörige \acs{smt} mittels der offiziellen Dokumentennummer der \acs{idta}, in der die betreffende Version spezifiziert ist.
Die in der Spalte Vorgesehene Inhalte genannten Punkte zeigen beispielhaft auf, welche Informationen im weiteren Verlauf im digitalen Zwilling abgebildet werden sollen.

\input{TabelleSubmodelle.tex}

\newpage
Im Rahmen dieser Arbeit ist die Modellierung der \acs{aas} bzw. der verschiedenen Submodelle als Instanz vorgesehen.
Auch wenn es sich bei dem Abfüll- und Verschließmodul grundsätzlich um ein generisches Modul innerhalb der robocell-Linie handelt und somit technisch gesehen als Typ klassifiziert werden könnte, steht in diesem Projekt die Umsetzung eines konkreten digitalen Zwillings im Vordergrund.
Dies begründet sich vor allem durch die vorgesehene Anbindung von Echtzeit- und Zeitreihendaten sowie die Abbildung einer Betriebsumgebung, was dem digitalen Zwilling einen klaren Instanzcharakter verleiht.
Zudem erleichtert dies die spätere Demonstration der \acs{aas} im Industrie-4.0-Systemkontext.

\subsection{Modellierung mit der AAS}
In diesem Abschnitt wird beschrieben, wie die zuvor ausgewählten Submodelle in eine \acs{aas} integriert und mit konkreten Daten sowie semantischen Informationen angereichert werden können.
Dabei wird erläutert, wie eine \acs{aas} von Grund auf modelliert, schrittweise mit Inhalten gefüllt und abschließend gespeichert bzw. exportiert werden kann.  

Die Umsetzung erfolgt mithilfe des Package Explorers, der eine intuitive Modellierung aller relevanten Elemente der \acs{aas} ermöglicht und so den strukturierten Aufbau eines digitalen Zwillings unterstützt.
Außerdem wird gezeigt, wie sich die erstellte \acs{aas} mit einer Test Engine auf eine korrekte und vollständige Struktur überprüfen lässt.

\subsubsection{Praktische Umsetzung mit dem Package Explorer}

Die Modellierung beginnt mit dem Erstellen eines neuen \acs{aas}-Pakets im Package Explorer.
Dieses dient als Container für die digitalen Inhalte eines Assets.  
In der geöffneten Umgebung lässt sich eine neue \acs{aas} anlegen, die sowohl allgemeine als auch assetspezifische Daten enthält. 
Abbildung~\ref{fig:NeuesAASPaket} zeigt den initialen Aufbau im Package Explorer, der im weiteren Verlauf sukzessive um Submodelle und deren Inhalte erweitert wird.
Dateien wie ein Titelbild für das Abfüll- und Verschließmodul können dabei bereits zu Beginn in die \acs{aas}-Umgebung eingebettet werden und sind ein integraler Bestandteil des digitalen Zwillings.

Innerhalb dieses Pakets müssen zunächst die Metadaten der \acs{aas} sowie des zugehörigen Asset spezifiziert werden.
Neben der Auswahl des Asset-Typs ist insbesondere die eindeutige Identifikation von zentraler Bedeutung.
Das Asset selbst wird über eine globalAssetId referenziert, während die \acs{aas} eine eigene \acs{id} und eine idShort erhält.
Diese vereinfachen nicht nur den späteren Austausch der \acs{aas}, sondern ermöglichen auch deren systemweites Auffinden innerhalb eines Industrie-4.0-Ökosystems.
Für erste Modellierungszwecke empfiehlt es sich, auf automatisch generierte Beispiel-IDs zurückzugreifen.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{Bilder/ModellierungAAS/Final/AASPaketPackageExplorer.PNG}
    \caption[Initiales \acs{aas}-Paket im Package Explorer]{Initiales \acs{aas}-Paket im Package Explorer}
    \label{fig:NeuesAASPaket}
\end{figure}



\subsubsection*{Strukturierung durch Submodelle}
\vspace{-0.5em}

Als nächstes müssen die benötigten Submodelle zur \acs{aas} hinzugefügt werden.
Diese lassen sich entweder als Instanz, Typ oder auch Template anlegen, was eine flexible Modellierung je nach Anwendungsfall ermöglicht.
Jedes Submodell erhält dabei wie die \acs{aas} auch eine eindeutige \acs{id}, über die diese später identifiziert werden können.
Zusätzlich können optionale Administrationsinformationen angegeben werden, wie beispielsweise die Versionsnummer oder der Name des Autors.

Beim Erstellen eines Submodells ist dieses zunächst leer. 
Die Modellierung erfolgt durch das schrittweise Hinzufügen von Submodellelementen.
Tabelle~\ref{tab:Submodellelemente} gibt einen Überblick über die wichtigsten Submodellelemente, die im Package Explorer zur Verfügung stehen.

% \vspace{-0.5em}
\input{TabelleSubmodellElemente}
\vspace{-0.5em}

Gemäß der Metamodellspezifikation \cite{SpezifikationPart1} wird zwischen DataElements und SubmodelElements unterschieden.
DataElements bilden stets die unterste Ebene im Modell. Sie enthalten konkrete Informationen wie Werte oder Dateien und können keine weiteren Elemente enthalten.
SubmodelElements hingegen sind komplexere Strukturen, die wiederum als Container für untergeordnete Elemente dienen.

Submodellelemente erhalten im Gegensatz zu Submodellen keine eigene \acs{id}, sondern werden über ihre idShort eindeutig referenziert.
Diese muss innerhalb eines Submodells einzigartig sein.
Eine Ausnahme bilden \acsp{sml}, in denen die enthaltenen Elemente nicht über eine idShort, sondern nur über ihre Position (Index) identifiziert werden.

Alternativ zur manuellen Modellierung können die im Rahmen der Konzeption ausgewählten \acsp{smt} verwendet werden, wie sie in Tabelle~\ref{tab:Submodelle} aufgeführt sind.
Diese stammen beispielsweise aus dem offiziellen Repository der \acs{idta} \cite{idtaTemplates} und lassen sich als AASX-Dateien importieren.
Im Package Explorer erfolgt dies über ein sogenanntes Auxiliary AAS, das als sekundäre Umgebung zur Template-Verwaltung dient.

Die importierten Templates enthalten eine strukturierte Vorlage mit typischen Submodell\-elementen und reduzieren somit den Modellierungsaufwand erheblich.
Auch unternehmensspezifische Ableitungen lassen sich darauf aufbauend erstellen.
Ein vertiefter Ansatz zur Template-Nutzung findet sich in Kapitel~\ref{chap:ErstellenvonSubmodelTemplates}.

Nach dem Strukturaufbau können konkrete Inhalte wie Werte, Dokumente oder Referenzen eingepflegt werden.
Dies beschränkt sich jedoch auf statische Submodelle wie Technische Daten oder Dokumentation.
Dynamische Inhalte, etwa Betriebsdaten, werden nicht direkt im Package Explorer gepflegt, sondern über externe Datenquellen (Maschinensimulator und Datengenerator) zur Laufzeit angebunden.

\subsubsection*{Semantische Beschreibung mit Concept Descriptions}
\vspace{-0.5em}

Für die semantische Beschreibung von Submodellen und Submodellelementen innerhalb der \acs{aas} ist die Vergabe von semanticId-Referenzen essenziell.
Jede semanticId verweist auf eine zugehörige \acs{cd}, die die Bedeutung des referenzierten Elements eindeutig und maschinenlesbar beschreibt.
Im Package Explorer lassen sich solche Concept Descriptions manuell anlegen und verwalten.
Dabei steht eine vordefinierte Eingabemaske zur Verfügung, die sich an dem in Part 3a: Data Specification - IEC 61360 \cite{SpezifikationPart3a} vordefinierten Datenmodell für semantische Beschreibungen orientiert.

Die Grundstruktur dieser Vorlage ist bei allen Elementen der \acs{aas} gleich.
Allerdings unterscheiden sich je nach Elementtyp die verpflichtenden und empfohlenen Felder.
Das bedeutet, dass für verschiedene Elemente (z.B. Property oder Relationship) jeweils unterschiedliche Felder als zwingend erforderlich gelten, während andere optional oder gar nicht vorgesehen sind.
Die Struktur wird im Folgenden am Beispiel einer Property für die physikalische Größe Frequenz dargestellt.

\vspace{0.25em}
\begin{figure}[htbp]
    \centering
    \input{DataSpecificationContent.tex}
    \vspace{-0.5em}
    \caption[Eingabemaske einer Concept Description]{Eingabemaske einer Concept Description (in Anlehnung an \cite{SpezifikationPart3a})}
    \label{fig:SubmodellTypenschild}
\end{figure}
\vspace{-0.35em}

Alternativ können auch externe Standards referenziert werden. 
Der Package Explorer unterstützt diesen Ansatz durch eine erweiterte Funktionalität, mit der sich vorgefertigte ECLASS-Kataloge direkt in das Tool importieren lassen. 
Nach dem Import können die enthaltenen Objekte durchsucht, ausgewählt und den entsprechenden Submodellen oder Submodellelementen zugewiesen werden. 
Die semantische Verknüpfung erfolgt dabei über eine \acs{irdi}, die das jeweilige Konzept eindeutig identifiziert \cite{eclass_irdi}. 

Wird ein solcher Standard referenziert, legt der Package Explorer automatisch eine \acs{cd} an.
Diese folgt, wie bei der manuellen Erstellung, dem in IEC~61360 definierten Datenmodell.  
So sind die semantischen Informationen auch lokal im Modell verfügbar und konsistent strukturiert.
Technisch betrachtet wäre jedoch auch eine rein externe Referenz ausreichend, da die entsprechenden Elemente bereits vollständig in den jeweiligen Katalogen enthalten sind.

Die Nutzung etablierter externer Standards ist grundsätzlich zu bevorzugen, da sie nicht nur den Modellierungsaufwand erheblich reduziert, sondern auch die Interoperabilität zwischen verschiedenen Systemen und Anwendungen deutlich verbessert.

\subsubsection*{Exportmöglichkeiten im Package Explorer}
\vspace{-0.5em}
Nach Abschluss der Modellierung lässt sich die erstellte \acs{aas} in verschiedenen Formaten exportieren.
Das bevorzugte Format in diesem Projekt ist das AASX-Format, das sich als standardisierte Austauschform für die \acs{aas} etabliert hat.
Es enthält alle modellierten Inhalte, einschließlich eingebetteter Dateien wie Bilder oder Dokumente, und eignet sich daher besonders für die Weitergabe einer Typ-1-\acs{aas}.

Alternativ kann die \acs{aas} auch als \ac{json}- oder \ac{xml}-Datei gespeichert werden.
Diese Formate beinhalten allerdings ausschließlich die strukturierte Beschreibung der \acs{aas}, eignen sich jedoch besonders für die Nutzung in \acsp{api} oder zur Anbindung an bestehende Softwaresysteme.

Zusätzlich unterstützt der Package Explorer auch den gezielten Export einzelner Submodelle oder Concept Descriptions.
Diese lassen sich seperat, etwa als \acs{json}-Datei, abspeichern und können so gezielt in anderen \acs{aas}-Projekten wiederverwendet werden.

\subsubsection{Validierung}
Im Anschluss an die Modellierung der \acs{aas} sollte eine Überprüfung der Konformität erfolgen.
Hierzu kann eine von der \acs{idta} bereitgestellte Test Engine \cite{TestEngine} eingesetzt werden. 
Diese lässt sich direkt mit pip, dem Paketmanager von Python, installieren und anschließend über die Kommandozeile nutzen.

Mit dem Befehl 
\tcbox[inlinebox]{{aas\_test\_engine check\_files}} 
kann die Validierung der zuvor erstellten \acs{aas} gestartet werden.
Unterstützt werden sowohl AASX- als auch \acs{json}-Dateien.
Dabei wird zunächst geprüft, ob diese formal korrekt aufgebaut sind, insbesondere hinsichtlich der internen Struktur und ihrer Beziehungen.
Daraufhin folgt die Kontrolle der enthaltenen \acs{aas} gegen die Metamodell-Spezifikationen (Teil~1 \cite{SpezifikationPart1} und%
\pagebreak 
~3a \cite{SpezifikationPart3a}).
Zuletzt erfolgt ein Abgleich der Submodelle mit den zugehörigen Templates, sofern diese für das jeweilige Submodell definiert wurden.

Treten bei der Validierung formale oder semantische Fehler auf, beispielsweise durch fehlende Referenzen oder ungültige IDs, gibt die Test Engine eine detaillierte Fehlermeldung in der Konsole aus. 
Diese Hinweise geben Aufschluss darüber, an welcher Stelle die Struktur bzw. der Inhalt der AASX-Datei fehlerhaft ist, und ermöglichen so eine gezielte Korrektur der betroffenen Elemente. 
Werden im gesamten Prüfprozess hingegen keine Fehler oder Abweichungen festgestellt, bestätigt die Test Engine die erfolgreiche Validierung.

\subsection{Technische Integration}
Im Anschluss an die Konzeption und Modellierung des digitalen Zwillings liegt der Fokus dieses Kapitels auf der technischen Integration in eine Industrie-4.0-kompatible Umgebung.
Dabei wird gezeigt, wie die statisch modellierte Typ-1-\acs{aas} mithilfe des AASX Server Blazor beziehungsweise vorzugsweise der Eclipse BaSyx-Plattform in eine Typ-2-\acs{aas} überführt und systemseitig bereitgestellt werden kann.
Ergänzend dazu wird beschrieben, wie die \acs{aas} um dynamische Inhalte erweitert werden kann.
Dies umfasst die Integration von Echtzeitdaten über \acs{opcua} sowie die Einbindung und Visualisierung externer Zeitreihendaten mithilfe von InfluxDB und Telegraf.

\subsubsection{Bereitstellung der \acs{aas}}
\label{sec:bereitstellungAAS}
Für die Bereitstellung als Typ-2-\acs{aas} stehen verschiedene Open-Source-Lösungen zur Verfügung. 
Eine besonders einsteigerfreundliche Option stellt der AASX Server Blazor \cite{AASXServer} dar.
Er bildet das serverseitige Gegenstück zum Package Explorer und verfügt ebenfalls über eine grafische Benutzeroberfläche zur Visualisierung von \acs{aas}-Paketen.

Über eine \acs{http}-Schnittstelle können die beiden Anwendungen miteinander verbunden werden.
Dadurch lassen sich AASX-Dateien ohne zusätzlichen Konfigurationsaufwand direkt aus dem Package Explorer heraus auf den Server übertragen und bereitstellen.
Ebenso können auf dem Server gespeicherte \acs{aas} über den Explorer eingesehen und bearbeitet werden.
Die enge Verzahnung beider Komponenten ermöglicht somit eine unkomplizierte Bereitstellung und Verwaltung von \acs{aas}-Paketen und eignet sich besonders für erste Testszenarien oder prototypische Anwendungen.

Für komplexere Anwendungsszenarien, insbesondere solche mit Anforderungen an Echtzeitfähigkeit sowie an flexible und erweiterbare Submodelle, stößt der%
\pagebreak
~AASX Server Blazor jedoch an seine funktionalen und architektonischen Grenzen. 
So fehlen beispielsweise Schnittstellen zur Anbindung dynamischer Datenquellen sowie Möglichkeiten zur Skalierung und zur verteilten Systemintegration.
Aus diesem Grund wird im weiteren Verlauf dieses Projekts die Eclipse BaSyx-Plattform eingesetzt.
Durch ihre modulare Architektur und die klare Trennung verschiedener Komponenten wie den Registries oder den Repositories bietet sie eine deutlich flexiblere Grundlage für die Umsetzung anspruchsvoller Industrie-4.0-Szenarien.

Die einfachste Möglichkeit, BaSyx zu installieren, besteht in der Nutzung von Docker.
Alle benötigten Komponenten stehen als vorgefertigte Images öffentlich über den Docker Hub \cite{BaSyxDockerHub} zur Verfügung. 
Alternativ kann der Quellcode von GitHub \cite{BaSyxGithub} bezogen werden, um einzelne Komponenten individuell anzupassen oder zu erweitern.
Für die vorliegende Arbeit ist dies insbesondere im Zusammenhang mit der AAS Web UI von Vorteil, da sich benutzerdefinierte Plugins flexibel integrieren lassen.

Die verschiedenen Services, darunter die AAS Web UI, die \acs{aas} Environment, Registries für \acs{aas} und Submodelle, der Discovery Service, sowie die MongoDB als persistenter Speicher, können zentral in einer docker-compose.yml-Datei verwaltet werden.
Die Konfiguration erfolgt über Umgebungsvariablen sowie separate Konfigurationsdateien, die in der docker-compose.yml referenziert werden.
% Mit dem Befehl docker compose up lassen sich anschließend alle Komponenten gemeinsam starten.

Nach erfolgreichem Start der BaSyx-Umgebung stehen verschiedene Möglichkeiten zur Verfügung, um eine \acs{aas} bereitzustellen und in das System zu integrieren. 
Welche Methode dabei zum Einsatz kommt, hängt stark von den jeweiligen Anforderungen und Rahmenbedingungen des Anwendungsszenarios ab.
Nachfolgend werden drei ausgewählte Ansätze zur Bereitstellung näher betrachtet, die in dieser Arbeit Anwendung finden.

\vspace{0.5em}
\noindent\textbf{A. Volume}\\
Eine besonders einfache Variante besteht darin, eine AASX-Datei in ein gemountetes Volume der AAS Environment abzulegen.
Ein Volume ist ein persistentes Speicherverzeichnis auf dem Host-System, das mit einem Verzeichnis innerhalb eines Docker-Containers verknüpft ist.
Es ermöglicht die dauerhafte Speicherung von Daten, unabhängig vom Lebenszyklus des Containers.
In der von Eclipse BaSyx bereitgestellten Docker-Umgebung ist ein entsprechendes Volume in der Regel bereits vorkonfiguriert.

Beim Neustart der AAS Environment wird die AASX-Datei automatisch erkannt, registriert und in das BaSyx-System eingebunden.
Die dabei erzeugten Daten, darunter Informationen zu \acs{aas}, Submodellen, Concept Descriptions sowie Registrierungsdaten, werden in verschiedenen Tabellen der angebunden MongoDB gespeichert.
Dies gewährleistet, dass alle relevanten Daten persistent erhalten bleiben, selbst dann, wenn die ursprüngliche AASX-Datei später wieder aus dem Volume entfernt wird.

\vspace{0.5em}
\noindent\textbf{B. AAS Web UI}\\
Alternativ kann die Bereitstellung auch direkt über die AAS Web UI erfolgen.
Über die Benutzeroberfläche kann eine AASX-Datei manuell importiert werden, wodurch sie direkt im laufenden System registriert und eingebunden wird.
Diese Methode eignet sich besonders gut für Tests oder kleinere Anpassungen, da eine \acs{aas} auf diese Weise schnell und ohne direkten Zugriff auf das zugrunde liegende Dateisystem eingebunden werden kann.

\vspace{0.5em}
\noindent\textbf{C. REST-API}\\
Die flexibelste, jedoch auch technisch anspruchsvollste Methode zur Bereitstellung ist die manuelle Registrierung über die \acs{rest}-\acs{api}. 
Dabei kann nicht einfach eine AASX-Datei hochgeladen werden. 
Stattdessen müssen die \acs{aas}, ihre Submodelle sowie deren Beziehungen explizit über die bereitgestellten Schnittstellen des BaSyx-Systems erstellt werden. 
Dies erfolgt durch das Senden strukturierter \acs{json}-Daten im Body der jeweiligen \acs{http}-Anfragen.
Ein typischer Ablauf dieser Registrierung ist in Tabelle \ref{tab:BereitstellungInBaSyx} dargestellt. 
Sie zeigt die notwendigen Schritte sowie die zugehörigen \acs{rest}-Endpunkte, die den jeweiligen Repositories der AAS Environment zugeordnet sind.

\input{TabelleBereitstellungBaSyxMitAPI.tex}

Neben der Bereitstellung in der AAS Environment kann eine \acs{aas} optional auch im Discovery Service registriert werden.
Über einen sogenannten assetLink kann sie dabei logisch mit ihrem zugehörigen physischen Asset verknüpft werden.
Die Registrierung erfolgt analog zur AAS Environment über einen \acs{rest}-Endpunkt.
Dies erleichtert insbesondere die eindeutige Zuordnung in komplexen, hierarchisch aufgebauten AAS, die wiederum mehrere verschachtelte Assets digital abbilden.

\subsubsection{Integration von Echtzeitdaten über OPC UA}

Nach der Erstellung der statischen \acs{aas} des Abfüll- und Verschließmoduls sowie deren Integration in das BaSyx-System soll diese nun um dynamische Informationen ergänzt werden.
Diese sind erforderlich, um den aktuellen Maschinenzustand präzise und in Echtzeit abzubilden.
Grundlage hierfür bilden die beiden zuvor beschriebenen Anwendungen, welche simulierte Maschinen- und Sensordaten über einen \acs{opcua} Server bereitstellen.

Die Integration der Echtzeitdaten in die \acs{aas} wird im Folgenden exemplarisch anhand des Submodells Prozessdaten erläutert.
Wie in Abbildung \ref{fig:SubmodellProzessdaten} dargestellt, enthält dieses Submodell verschiedene Properties, die jeweils spezifische Werte, wie etwa den Druck oder die Anzahl abgefüllter Einheiten, repräsentieren.
Diese Properties sollen im weiteren Verlauf dynamisch mit den über \acs{opcua} bereitgestellten Werten aktualisiert werden.

\begin{figure}[htbp]
    \centering
    % 0.88
    \includegraphics[width=1\textwidth]{Bilder/OPCUA/ProcessData.pdf}
    \caption[Struktur Submodell Prozessdaten]{Struktur Submodell Prozessdaten}
    \label{fig:SubmodellProzessdaten}
\end{figure}
\vspace{-0.5em}

Das Eclipse BaSyx-Projekt stellt hierfür eine weitere Komponente bereit, die sogenannte Databridge \cite{BaSyxDatabridge}.
Diese steht, wie alle anderen Komponenten, als Docker-Container zur Verfügung und ermöglicht die Anbindung verschiedenster Datenquellen an eine \acs{aas}.
Sie unterstützt eine Vielzahl von Protokollen, darunter insbesondere \acs{opcua} und \acs{mqtt} (Message Queuing Telemetry Transport).
Dabei dient sie als Vermittler zwischen einem Datenendpunkt und einem Submodell innerhalb der \acs{aas}.

Die Konfiguration der Databridge erfolgt über mehrere \acs{json}-Dateien.
In einer zentralen Konfigurationsdatei werden sowohl die Datenquelle als auch die Datensenke definiert.
Zusätzlich können Transformatoren angegeben werden, die beispielsweise Einheitenumrechnungen oder Typkonvertierungen übernehmen.

Im Kontext des Submodells Prozessdaten erfolgt die Anbindung der simulierten Werte über den \acs{opcua} Server des Datengenerators, der als Datenquelle für die Databridge dient.
Die Konfiguration erfolgt in einer separaten \acs{json}-Datei.
Darin sind sowohl die Verbindungsparameter des Servers (z.~B. \acs{url} (Uniform Resource Locator) und Port) als auch die zu überwachenden Knoten anzugeben.
Wie in Abbildung \ref{fig:OPCUADatenStruktur} zu erkennen, stellt der \acs{opcua} Server die Werte hierfür in einer hierarchischen Struktur bereit, wobei jeder Knoten über einen Namespaceindex (ns) und eine NodeId (i) eindeutig adressierbar ist.

\newpage
\begin{figure}[htbp]
    \centering
    % 0.88
    \includegraphics{Bilder/OPCUA/OPCUADaten.pdf}
    \caption[\acs{opcua} Datenknoten des Datengenerators]{\acs{opcua} Datenknoten des Datengenerators}
    \label{fig:OPCUADatenStruktur}
\end{figure}

Darüber hinaus muss der zu verwendende \acs{opcua} Client festgelegt werden.
In diesem Projekt kommt Eclipse Milo zum Einsatz, der auf einem Subscription-Modell basiert.
Im Gegensatz zu einem Polling-Ansatz werden hierbei gezielt bestimmte Knoten abonniert, sodass neue Werte bei Änderungen automatisch übermittelt werden.

Ein Beispiel für eine entsprechende \acs{json}-Konfiguration der Datenquelle ist in Listing~\ref{lst:jsonDatenquelle} für den Druckwert dargestellt.
Weitere optionale Parameter, wie etwa Sicherheitseinstellungen oder das Übertragungsintervall können zusätzlich angegeben werden, wurden hier jedoch zur besseren Übersicht weggelassen.

\begin{lstlisting}[language=json, caption={Beispielhafte \acs{json}-Konfiguration einer Datenquelle}, label={lst:jsonDatenquelle}]
{
    "uniqueId"       : "pressure",
    "nodeInformation": "ns=4;i=113",
    "serverUrl"      : "opcua-server",
    "serverPort"     : 4840,
    "pathToService"  : "milo"
}
\end{lstlisting}

Zur Anpassung der eingehenden Daten an die Struktur der Ziel-Property können verschiedene Transformatoren eingesetzt werden.
Die Databridge unterstützt hierfür unter anderem JSONata-Ausdrücke sowie Jackson-basierte Transformer.
Mithilfe dieser Mechanismen lassen sich die vom \acs{opcua} Server empfangenen Rohdaten zunächst in ein \acs{json}-Objekt überführen und anschließend gezielt extrahieren sowie weiterverarbeiten.

Im Anschluss erfolgt die Konfiguration der Datensenke, also der Zielkomponente innerhalb der \acs{aas}.
Auch diese erfolgt über eine separate \acs{json}-Datei, in der der Endpunkt des Submodells, der idShortPath der gewünschten Property sowie die verwendete \acs{api}-Version definiert werden.
Listing~\ref{lst:jsonDatensenke} zeigt ein Beispiel einer entsprechenden Konfiguration.
Der Platzhalter \{smId\} steht dabei für die Base64-kodierte \acs{id} des Submodells Prozessdaten, wie sie in der \acs{rest}-API der \acs{aas} verwendet wird.

\newpage
\begin{lstlisting}[language=json, caption={Beispielhafte \acs{json}-Konfiguration einer Datensenke}, label={lst:jsonDatensenke}]
{
    "uniqueId"        : "Submodel/ProcessData/Pressure",
    "submodelEndpoint": "http://aas-env:8081/submodels/{smId}",
    "idShortPath"     : "Data.Pressure",
    "api"             : "DotAAS-V3"
}
\end{lstlisting}

Nach erfolgreicher Konfiguration übernimmt die Databridge die automatisierte Übertragung der \acs{opcua} Werte in die entsprechenden Properties des Submodells.
Dieses Prinzip lässt sich nicht nur auf Prozesswerte anwenden, sondern auch zur Abbildung des Maschinenzustands oder anderer dynamischer Betriebsdaten nutzen.
Damit stellt die Databridge eine zentrale Komponente zur Echtzeitanbindung externer Datenquellen dar und trägt wesentlich zur dynamischen Erweiterung der \acs{aas} bei.

\subsubsection{Verarbeitung von Zeitreihendaten}
\label{sec: VerarbeitungZeitreihen}
Grundsätzlich lassen sich Zeitreihendaten auf unterschiedliche Weise in eine \acs{aas} integrieren.
Das \acs{smt} Time Series Data \cite{SpezifikationTimeSeriesData} bietet hierfür mehrere standardisierte Lösungsansätze.
Eine Möglichkeit besteht darin, die Daten direkt innerhalb der \acs{aas} zu speichern. 
Dies erfolgt über ein InternalSegment, das die entsprechenden Einträge enthält.
Diese Variante eignet sich jedoch nur für kleinere Datenmengen.

Alternativ können die Zeitreihendaten in Form einer Datei abgelegt werden. 
Diese Datei kann entweder direkt in die \acs{aas} eingebunden oder über ein ExternalSegment referenziert werden.
Für größere Datenmengen empfiehlt sich die externe Speicherung an einem separaten Ort, beispielsweise in einer Datenbank.
Die Verknüpfung mit der \acs{aas} erfolgt in diesem Fall über ein LinkedSegment.

In dieser Arbeit wird die zuletzt genannte Option näher untersucht und praktisch umgesetzt.
Hierzu werden die simulierten Werte für Druck und Temperatur des Datengenerators extern in einer InfluxDB gespeichert.
Die über \acs{opcua} bereitgestellten Daten werden mithilfe von Telegraf, einem leichtgewichtigen Agenten zur Datenerfassung und -weiterleitung \cite{Influx}, kontinuierlich in eine speziell für diesen Anwendungsfall angelegte Tabelle geschrieben.
InfluxDB sowie Telegraf stehen beide als Docker-Container zur Verfügung und lassen sich so nahtlos in das bestehende BaSyx-System integrieren.

Zur Einbindung der in der Datenbank gespeicherten Daten in die \acs{aas} kann das bereits genannte \acs{smt} verwendet werden.
Abbildung \ref{fig:SMTTimeSeriesData} zeigt den strukturellen Aufbau, wobei ausschließlich die für die externe Anbindung relevanten Elemente berücksichtigt sind.

\begin{figure}[htbp]
    \centering
    % 0.88
    \includegraphics[width=1\textwidth]{Bilder/TimeSeries/TimeSeriesData.pdf}
    \caption[Struktur \acs{smt} Time Series Data]{Struktur \acs{smt} Time Series Data (in Anlehnung an \cite{SpezifikationTimeSeriesData})}
    \label{fig:SMTTimeSeriesData}
\end{figure}

Zuerst müssen die Metadaten eingetragen werden. 
Dazu gehören ein eindeutiger Name, eine Beschreibung sowie die Festlegung der Struktur der aufzuzeichnenden Datenpunkte in der \acs{smc} Records.
Ein Record kann mit einer Spaltenbeschreibung in einer Tabelle verglichen werden.
Er beschreibt, welche Variablen, in diesem Fall Druck und Temperatur, ein Zeitreihendatensatz enthält und wie diese zu interpretieren sind.

Die Konfiguration des LinkedSegments bildet den nächsten Schritt.
Dieses stellt die eigentliche Verbindung zu den extern gespeicherten Zeitreihendaten her. 
Hierfür sind sowohl der Datenendpunkt als auch die Abfrage zu spezifizieren, mit der die gewünschten Werte ausgelesen werden können. 
Im vorliegenden Fall handelt es sich um die Adresse des InfluxDB-Containers sowie die Abfrage, mit der die Werte für Druck und Temperatur aus der zugehörigen Tabelle extrahiert werden. 
Dabei ist zu beachten, dass der entsprechende Port der InfluxDB nach außen freigegeben ist, da andernfalls kein Zugriff durch externe Anwendungen möglich ist.

Ergänzend können weitere Metadaten angegeben werden, beispielsweise die Abtastrate, der durch das Segment abgedeckte Zeitraum oder ein recordCount, der die erwartete Anzahl an Einträgen innerhalb dieses Zeitfensters beschreibt.
Alle relevanten Informationen zur externen Speicherung der Zeitreihendaten sind somit im Submodell enthalten, wodurch externen Clients ein strukturierter und standardisierter Zugang zu den Daten ermöglicht wird.

\newpage
\subsection{KI-Modell zur Optimierung}
In diesem Abschnitt wird ein Konzept zur Optimierung der zustandsbasierten Prozessüberwachung auf Basis von \acs{ki} vorgestellt und prototypisch umgesetzt.
Ziel ist es, mithilfe maschinellen Lernens potenzielle Anomalien in den im digitalen Zwilling abgebildeten Prozessdaten zu erkennen.
Damit wird eine Grundlage für proaktive Instandhaltungsmaßnahmen geschaffen, die zur Optimierung des Betriebs einer Maschine beitragen können.


% und dadurch eine Grundlage für prädiktive Instandhaltungsmaßnahmen zu schaffen.
% Darüber hinaus wird gezeigt, wie das resultierende \acs{ki}-Modell mithilfe der \acs{aas} standardisiert und interoperabel verwaltet werden kann.

\subsubsection{Konzeptidee}
Mit der fortschreitenden Digitalisierung industrieller Prozesse entstehen zunehmend umfangreiche Datenmengen, die wertvolle Informationen über den Zustand und das Verhalten eines Assets liefern. 
Dabei handelt es sich um unterschiedlichste Datenarten, etwa die Drehzahl eines Motors oder, wie im Rahmen dieser Arbeit im digitalen Zwilling abgebildet, klassische Prozessdaten wie Druck und Temperatur.
Die manuelle Analyse dieser Daten kann sich als sehr herausfordernd gestalten. Abweichungen vom Normalbetrieb sind oft schwer zu erkennen, was eine zuverlässige Zustandsüberwachung erheblich erschwert.

Ein vielversprechender Ansatz, der genau diese Problematik adressiert, ist der sogenannte Autoencoder. 
Dieser stellt eine spezielle Architektur tiefer neuronaler Netze dar und besteht typischerweise aus zwei Hauptkomponenten: einem Encoder und einem Decoder, die gemeinsam eine komprimierte, latente Repräsentation der Eingangsdaten erlernen.
\cite{Lempitsky2019}

Der prinzipielle Aufbau eines Autoencoders ist in Abbildung \ref{fig:Autoencoder} dargestellt.

\begin{figure}[htbp]
    \centering
    % 0.88
    \includegraphics[width=1\textwidth]{Bilder/Autoencoder/AutoencoderModell.pdf}
    \caption[Aufbau eines Autoencoders]{Aufbau eines Autoencoders (in Anlehnung an \cite{AutoencoderBild})}
    \label{fig:Autoencoder}
\end{figure}

\vspace{-0.75em}
Der Encoder besteht aus mehreren Schichten, die die Eingabedaten schrittweise in%
\pagebreak
~eine komprimierte Form überführen. 
Dabei erfolgt eine Dimensionsreduktion, das heißt, die Daten werden im Verlauf auf weniger Dimensionen verdichtet. 
Das Ziel ist es, die für die Rekonstruktion wesentlichen Merkmale zu extrahieren und irrelevante Informationen zu filtern.

Der dabei entstehende latente Raum bildet die am stärksten komprimierte Repräsentation der Daten ab und dient als Ausgangspunkt für den Decoder.
Dieser übernimmt die Aufgabe, die komprimierte Darstellung wieder in die ursprüngliche Eingabe zu rekonstruieren. 
Auch er besteht aus mehreren Schichten, die die Daten schrittweise dekomprimieren.
Zur Bewertung der Leistungsfähigkeit kann die resultierende Ausgabe mit der ursprünglichen Eingabe verglichen werden. 
Die Differenz zwischen beiden wird als Rekonstruktionsfehler bezeichnet. \cite{Autoencoder}

Im Rahmen dieser Arbeit soll ein Autoencoder zur Erkennung von Anomalien in Zeitreihendaten eingesetzt werden. 
Geplant ist ein Training mit Daten aus dem Normalbetrieb. 
Da diese nicht gelabelt sind, wird ein unüberwachter Lernansatz verfolgt. 
Ziel ist es, dass das Modell typische Muster des Normalbetriebs eigenständig erkennt und rekonstruieren kann. 
Weichen neue Eingangsdaten deutlich vom gelernten Muster ab, steigt der Rekonstruktionsfehler.
Dieser dient als Indikator für eine potenzielle Anomalie.

\subsubsection{Prototypische Umsetzung}
Für die prototypische Umsetzung des Autoencoders wird die Programmiersprache Python verwendet. 
Aufgrund ihrer Vielzahl an spezialisierten Frameworks für maschinelles Lernen gilt sie als Standardwerkzeug für datengetriebene Analysen \cite{Python}.
Als Datengrundlage dienen die simulierten Temperaturverläufe, die mithilfe des Datengenerators erzeugt und, wie auch in Kapitel \ref{sec: VerarbeitungZeitreihen} beschrieben, in einer InfluxDB gespeichert sind.

Im ersten Schritt gilt es, die für das Training notwendigen Daten vorzubereiten. 
Hierzu kann ein InfluxDB-Client eingesetzt werden, der über ein \acs{api}-Token sowie die \acs{url} des Containers mit der Datenquelle kommuniziert.
Die relevanten Temperaturdaten lassen sich anschließend durch gezielte Abfragen aus der entsprechenden Tabelle extrahieren.
In dieser Umsetzung wird dabei ein Zeitraum von 7 Tagen betrachtet.

Um dem Autoencoder das Erkennen typischer Muster innerhalb dieser Daten zu ermöglichen, müssen diese zunächst in Segmente unterteilt werden.
Die Wahl der Fenstergröße spielt dabei eine entscheidende Rolle, da sie die Fähigkeit des Modells beeinflusst, relevante Prozessänderungen zu erfassen.
Da diese Änderungen in der Regel innerhalb kurzer Zeitabschnitte auftreten, ist es wichtig, eine geeignete Fensterlänge zu wählen, die ausreichend Kontext liefert, ohne zu große Zeiträume zusammenzufassen.


% Da relevante Prozessänderungen typischerweise innerhalb kurzer Zeitabschnitte auftreten ein kleiner Abschnitt
% , wird in dieser Arbeit eine Fenstergröße von 30 Werten gewählt. 
% Bei einer Abtastrate von 1 Hz entspricht dies einem Zeitraum von 30 Sekunden.

Die resultierenden Sequenzen können anschließend in einem Tensor zusammengeführt werden. 
Dabei handelt es sich um ein mehrdimensionales Array, in diesem Fall eine zweidimensionale Matrix, bei der jede Zeile ein Zeitfenster mit einer definierten Anzahl an Temperaturwerten abbildet. 
Diese Struktur ist speziell für \acs{ki}-Modelle ausgelegt und erlaubt es dem Autoencoder, typische Muster in den Zeitreihendaten zu erlernen.

Der Autoencoder selbst wird mit dem Deep-Learning-Framework PyTorch \cite{PyTorch} implementiert.
Der Encoder komprimiert die Eingabesequenz schrittweise von 64 auf 16 und schließlich auf 4 Neuronen, welche den latenten Raum repräsentieren.
Der Decoder ist spiegelbildlich aufgebaut und rekonstruiert aus dem latenten Raum die ursprüngliche Eingabesequenz, indem er die Dimensionen in umgekehrter Reihenfolge auf 16, 64 und schließlich auf die ursprüngliche Sequenzlänge erweitert.

Als Eingabe für das Training dienen die zuvor definierten Tensoren.
Das Ziel ist es, die Eingabedaten möglichst exakt zu rekonstruieren.
Nach jedem Trainingsdurchgang wird hierzu der mittlere quadratische Fehler zwischen Eingabe- und Ausgabedaten berechnet.
Dieser Fehlerwert wird verwendet, um die Gewichte des Netzes, typischerweise mittels Backpropagation, anzupassen.
Durch die iterative Wiederholung dieses Prozesses über mehrere Trainingszyklen (Epochen) wird der Rekonstruktionsfehler sukzessive verringert und somit die Fähigkeit des Modells, die Eingabedaten genau nachzubilden, kontinuierlich verbessert.

Das trainierte Modell kann anschließend zur Erkennung von Anomalien in neuen Daten genutzt werden.
Dabei dient der Rekonstruktionsfehler nicht mehr zur Optimierung, sondern als Maß für Abweichungen von der Normalität.
Überschreitet der Fehler eines Zeitfensters einen definierten Schwellenwert, so liegt eine Anomalie vor.

\newpage
% Seitenumbruch im Inhaltsverzeichnis
% \addtocontents{toc}{\protect\newpage}
\subsection{Anwendungsfall Digitaler Produktpass}
Angesichts steigender Anforderungen an Nachhaltigkeit und Transparenz über den gesamten Produktlebenszyklus gewinnt der \acs{dpp} zunehmend an Bedeutung.
Aufbauend auf dem von \acs{zvei} und \acs{idta} vorgestellten Konzept des \acs{dpp40}, das unter anderem in einem \acs{pcf}-Showcase \cite{PCFShowcas} demonstriert wird, wird in diesem Anwendungsfall untersucht, wie sich zentrale Herausforderungen mithilfe der \acs{aas} umsetzen lassen.
Im Mittelpunkt stehen dabei die Erfassung des \acs{pcf} sowie die Realisierung differenzierte Zugriffsrechte auf die im \acs{dpp} enthaltenen Informationen.
Grundlage bildet die zuvor erstellte \acs{aas} des Abfüll- und Verschließmoduls, das um ein Submodell zur Abbildung des \acs{cf} erweitert wird.

\subsubsection{Umsetzung mit dem \acs{smt} Carbon Footprint}
Der \acs{pcf} beschreibt die Summe aller Treibhausgasemissionen, ausgedrückt in CO\textsubscript{2}-Äqui\\valenten, die entlang des Lebenszyklus eines Produkts entstehen \cite{PCF}. 
Seine Relevanz zeigt sich unter anderem in der EU-Batterieverordnung \cite{EUVerordnung}, die die Einführung eines digitalen Batteriepasses vorschreibt. 
Dieser stellt die erste konkrete Umsetzung eines \acs{dpp} dar und dient als Ausgangspunkt für weitere Branchen wie den Maschinen- und Anlagenbau. 
Die Verordnung fordert unter anderem die verpflichtende Angabe des \acs{pcf} für die Phasen Material, Produktion und die Gesamtbetrachtung (Cradle to Gate). 
Für weiterführende Lebenszyklusphasen ist die Angabe derzeit noch nicht vorgeschrieben.

Für die Umsetzung bietet sich das von der \acs{idta} spezifizierte \acs{smt} \acs{cf} \cite{SpezifikaitonPCF} an.
Es definiert eine standardisierte Struktur zur Erfassung von CO\textsubscript{2}-Äquivalenten und unterscheidet zwischen dem \acs{pcf} und dem \ac{tcf}, wobei in diesem Anwendungsfall ausschließlich der \acs{pcf} betrachtet wird.
Die Modellierung erfolgt über \acsp{smc}, die standardisierte Elemente wie den CO\textsubscript{2}-Wert, die betrachtete Lebenszyklusphase, die Berechnungsmethode sowie den Gültigkeitszeitraum enthalten.
Für jede relevante Phase empfiehlt es sich, eine eigene \acs{smc} anzulegen.
Die konkreten Werte bleiben zunächst leer und werden im weiteren Verlauf dynamisch gefüllt.

Zur Demonstration wurde eine Auswahl von acht im Abfüll- und Verschließmodul der robocell verbauten Siemens-Steuerungskomponenten getroffen, darunter beispielsweise eine CPU sowie Ein- und Ausgangsmodule.
Die Entscheidung für Siemens basiert darauf, dass für deren Produkte bereits \acs{aas}-Strukturen mit nachhaltigkeitsbezogenen Informationen vorliegen, auch wenn diese derzeit noch nicht öffentlich verfügbar sind.
Die zugehörigen \acs{aas} wurden im Rahmen des Projekts auf persönliche Anfrage von der Siemens AG bereitgestellt. 
Dabei enthalten sie jeweils ein eigenes Submodell gemäß dem \acs{smt} \acs{cf}.

\clearpage
Die Referenzierung der Komponenten erfolgt im \acs{cf}-Submodell der \acs{aas} des Abfüll- und Verschließmoduls, nachfolgend als Haupt-\acs{aas} bezeichnet.
Dies geschieht über eine \acs{sml}, in der die ausgewählten Einheiten als Self-Managed Entities unter Angabe ihrer globalAssetId eingebunden sind.
Self-Managed Entities sind eigenständige Assets, die über eine eigene \acs{aas} verfügen und somit unabhängig adressierbar sind.
Zwar wäre auch eine Referenzierung über das \acs{bom}-Submodell denkbar, jedoch wären dadurch die für die Berechnung relevanten Daten auf verschiedene Submodelle verteilt, was die Aggregation erschweren würde.

Die aktualisierte Haupt-\acs{aas} sowie die zugehörigen Komponenten-\acs{aas} werden über die Eclipse~BaSyx-Plattform bereitgestellt und bilden die Grundlage für die technische Umsetzung.
Zur Benutzerinteraktion dient das bestehende Vue.js-Plugin der AAS Web UI, das um eine Funktion zur Ermittlung der Gesamt-\acs{pcf}-Werte erweitert wurde.
Zur Trennung von Logik und Benutzeroberfläche wird ein eigenständiger, Node.js-basierter Microservice implementiert, der eine Web-\acs{api} bereitstellt.
Über diese kann die Berechnung der CO\textsubscript{2}-Äquivalente ausgelöst werden.
Die Aggregation erfolgt damit zentral und serverseitig, anstatt direkt im Plugin clientseitig.

Der Microservice liest die im \acs{cf}-Submodell des Abfüll- und Verschließmoduls referenzierten Komponenten aus, fasst deren \acs{pcf}-Werte zusammen und schreibt die berechneten CO\textsubscript{2}-Äquivalente in die vorbereiteten \acsp{smc} der Haupt-\acs{aas} zurück.
Zusätzlich können weitere Metadaten wie die verwendete Berechnungsmethode oder der Gültigkeitszeitraum berücksichtigt werden.
Da diese Werte im vorliegenden Anwendungsfall jedoch bei allen Komponenten identisch sind, ist keine Differenzierung erforderlich.

\subsubsection{Zugriffsrechte und Datensicherheit}

Die kontrollierte Bereitstellung der im \acs{dpp} enthaltenen Informationen ist essenziell, insbesondere im Hinblick auf Datenschutz, regulatorische Vorgaben sowie den Schutz geistigen Eigentums.  
Wie in Kapitel~\ref{sec: Sicherheit} beschrieben, sieht die Security-Spezifikation der \acs{idta} hierfür die Nutzung eines attributbasiertes Zugriffskontrollmodell (\acs{abac}) vor.

Da die \acs{abac}-Funktionalität in aktuellen Referenzimplementierungen wie dem AASX Server Blazor oder Eclipse BaSyx jedoch noch nicht vollständig verfügbar ist, wird im Rahmen dieses Anwendungsfalls exemplarisch ein rollenbasiertes Zugriffsmodell (\acs{rbac}) umgesetzt, wie es von der Eclipse BaSyx-Plattform unterstützt wird.  
Im Gegensatz zum \acs{abac}-Ansatz erfolgt die Zugriffskontrolle hierbei durch die Zuweisung spezifischer Berechtigungen an vordefinierte Rollen, denen wiederum Benutzer oder technische Clients zugeordnet sind.

Zur Umsetzung von Authentifizierung und Autorisierung wird der Open-Source Identity Provider Keycloak eingesetzt \cite{Keycloak}.  
Dieser ermöglicht die zentrale Verwaltung von Benutzern, Rollen und Clients und unterstützt eine tokenbasierte Zugriffskontrolle auf Basis von \acsp{jwt}.  
Keycloak steht als Docker-Container zur Verfügung und kann nahtlos in die BaSyx-Systemarchitektur integriert werden.

Im ersten Schritt muss ein dediziertes Realm eingerichtet werden, das als gekapselte Umgebung für alle projektbezogenen Identitäten dient.  
Innerhalb dieses Realms lassen sich sowohl Benutzerkonten als auch Rollen definieren, wobei die Zuweisung über sogenannte Role Mappings erfolgt.  
Ein Benutzer kann einer oder mehreren Rollen gleichzeitig zugeordnet sein.

Jedes Benutzerkonto verfügt dabei über einen eindeutigen Benutzernamen sowie ein initiales Passwort zur Authentifizierung.  
Zur Erhöhung der Sicherheit kann dieses Passwort als temporär markiert werden, wodurch der Benutzer bei der ersten Anmeldung zu einer Änderung gezwungen wird.  
Darüber hinaus bietet Keycloak ebenfalls die Möglichkeit, sogenannte Required Actions (erforderliche Aktionen) zu definieren, etwa zur E-Mail-Verifizierung oder zur erzwungenen Passwortänderung beim nächsten Login.  
Dies erlaubt eine flexible Anpassung des Authentifizierungsprozesses an projektspezifische Sicherheitsanforderungen.

Neben der Verwaltung menschlicher Benutzer unterstützt Keycloak auch die Administration technischer Clients, die z.B. maschinelle Anwendungen oder externe Systeme repräsentieren.  
Die Authentifizierung erfolgt in der Regel über eine Kombination aus eindeutiger Client-\acs{id} und einem geheimen Schlüssel (Client Secret) gemäß dem OpenID Connect-Protokoll \cite{OpenID}.  
Für jeden Client kann optional ein Service Account aktiviert werden, der als technisches Benutzerkonto agiert und, analog zu realen Benutzern, mit spezifischen Rollen ausgestattet ist.  
Dies ermöglicht eine differenzierte Zugriffsteuerung auch für automatisierte Systemzugriffe.

Die Rechtevergabe innerhalb der einzelnen BaSyx-Komponenten erfolgt rollenbasiert über externe Konfigurationsdateien im \acs{json}-Format.  
Darin wird definiert, welch Rollen welche Berechtigungen (z.~B. READ, WRITE, DELETE ) auf bestimmte Elemente wie \acs{aas}, Submodelle oder Concept Descriptions erhalten.  
Für generische Regeln kann der Platzhalter \texttt{*} verwendet werden, um beispielsweise den Zugriff auf alle Submodelle einer BaSyx-Komponente zu gewähren.  
Die AAS Web UI benötigt keine eigene \acs{rbac}-Datei, sondern orientiert sich an den Konfigurationen der angebundenen Services.

\subsection{Anwendungsfall automatisierte Generierung von AAS}
Im folgenden Anwendungsfall wird gezeigt, wie eine \acs{aas} automatisiert erstellt und bereitgestellt werden kann.  
Ausgangspunkt ist ein unternehmensspezifisch angepasstes \acs{smt}, das als Grundlage für die Ableitung eines Typ-Submodells dient.  
Dieses wird anschließend mit konkreten Daten befüllt, in eine vollständige \acs{aas}-Instanz eingebettet und abschließend in das Eclipse BaSyx-System integriert.

\subsubsection{Arbeiten mit Submodel Templates}
\label{chap:ErstellenvonSubmodelTemplates}
Für die automatisierte Generierung einer \acs{aas} ist eine konsistente Submodellstruktur erforderlich, die sich aus wiederverwendbaren Vorlagen ableiten lässt.
Im Rahmen dieses Anwendungsfalls dient das standardisierte \acs{smt} Technische Daten \cite{SpezifikaitonTechnischeDaten} als Grundlage. 
Es definiert eine generische Struktur zur Beschreibung technischer Merkmale, gegliedert in Kategorien (\acsp{smc}) wie Generelle Informationen, Technische Informationen oder Produktklassifikation. 
Damit bildet es den semantischen Rahmen, enthält jedoch zunächst keine konkreten Ausprägungen.

Auf Unternehmensebene kann das generische Template an spezifische Anforderungen angepasst werden, beispielsweise für einen bestimmten Maschinentyp wie die robocell.
Branchenspezifische Varianten sind grundsätzlich ebenfalls denkbar, werden in diesem Anwendungsfall jedoch nicht weiter betrachtet.

Die Anpassung erfolgt mithilfe des Package Explorers, über den sich das \acs{smt} importieren und erweitern lässt.
So können beispielsweise produktspezifische Anforderungen, wie Umgebungsbedingungen oder der Verarbeitungsbereich, mithilfe geeigneter Submodellelemente ergänzt werden.
Das resultierende unternehmensspezifische Template erhält eine eigene semanticId, bleibt jedoch zusätzlich mit der ursprünglichen semanticId des generischen Templates verknüpft, um die Rückverfolgbarkeit zur standardisierten Vorlage sicherzustellen.

Aus dem angepassten Template kann anschließend ein Typ-Submodell abgeleitet werden, das nicht nur die Struktur, sondern auch allgemeingültige Merkmale einer Produktgruppe enthält, etwa den Einsatzort oder die maximal zulässige Umgebungstemperatur.
Dieses Typ-Submodell dient als Vorlage für die spätere Erstellung konkreter \acs{aas}-Instanzen, die mit produktspezifischen Werten wie Seriennummern ergänzt werden.
Die Verbindung zur ursprünglichen sowie zur unternehmensspezifischen Template-Struktur bleibt dabei über entsprechende semanticId-Verknüpfungen erhalten.

\subsubsection{Automatisiertes Befüllen mit strukturierten Daten}

Nach der Ableitung eines Typ-Submodells stellt sich die Frage, wie dieses effizient mit konkreten Produktinformationen befüllt und in eine vollständige \acs{aas}-Instanz überführt werden kann.

In der industriellen Praxis liegen die benötigten Daten häufig bereits strukturiert in bestehenden \acs{it}-Systemen wie \acs{plm}- oder \acs{erp}-Lösungen vor.
Da eine direkte Systemintegration im Rahmen dieser Arbeit jedoch nicht realisierbar ist, wird der Prozess anhand vordefinierter Beispieldaten demonstriert.
Diese enthalten typische technische Merkmale, wie sie auch in realen Unternehmenssystemen vorzufinden sind, und bilden die Grundlage für die automatisierte Erstellung eines Instanz-Submodells.

Die automatisierte Befüllung erfolgt mithilfe eines Skripts, das die Beispieldaten in die vorgegebene Submodellstruktur überträgt und in eine neue \acs{aas}-Instanz einbettet.
Zum Einsatz kommt die serverseitige JavaScript-Laufzeitumgebung Node.js, da sie eine effiziente Verarbeitung von \acs{json}-Daten sowie eine unkomplizierte Kommunikation mit \acs{rest}-Schnittstellen ermöglicht \cite{nodejs}.

Die Basis bilden drei strukturierte \acs{json}-Dateien:

\begin{enumerate}[noitemsep, leftmargin=*, label=\textbf{\arabic*.}]
    \item \textbf{Datenquelle:} Technische Produktinformationen
    \item \textbf{Typ-Submodell:} Struktur und Semantik des Submodells
    \item \textbf{AAS-Vorlage:} Aufbau und Struktur der \acs{aas}-Instanz
\end{enumerate}

Die Datenquelle ist hierarchisch aufgebaut und enthält verschachtelte Schlüssel-Wert-Paare.
Jeder Schlüssel entspricht einem idShort-Wert eines Submodellelements innerhalb des Typ-Submodells.
Dieses ist so konzipiert, dass Platzhalter an den relevanten Stellen eingefügt sind, die bei der Skriptausführung durch die zugehörigen Werte aus der Datenquelle ersetzt werden.

Das resultierende Instanz-Submodell muss anschließend in eine \acs{aas}-Instanz eingebunden werden.
Die AAS-Vorlage definiert hierfür zwar die grundlegende Struktur, enthält jedoch noch keine spezifischen Identifikatoren wie die \acs{id} der \acs{aas} oder des zugehörigen Assets.
Um diese hinzuzufügen, können während der Skriptausführung zufällig generierte UUID-Werte (Universally Unique Identifier) an den entsprechenden Stellen in die Vorlage eingefügt werden.

Im letzten Schritt muss die \acs{aas}-Instanz innerhalb des Eclipse BaSyx-Systems bereitgestellt werden.
Hierzu kann die \acs{aas} sowie das zugehörige Submodell, analog zur Möglichkeit~C \acs{rest}-API aus Kapitel~\ref{sec:bereitstellungAAS}, eingebunden und registriert werden.